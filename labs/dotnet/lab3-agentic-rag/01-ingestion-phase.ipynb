{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632f6abc",
   "metadata": {},
   "source": [
    "# Notebook 1: The Ingestion Phase\n",
    "\n",
    "As we discussed in the presentation, the ingestion phase is basically the loading of the data sources the retrieval system uses. These data sources can be existing databases with structured data, however in this notebook we'll focus on unstructured data (such as documents).\n",
    "\n",
    "## Learning Objectives\n",
    "- Learn how to chunk markdown files into smaller sizes\n",
    "- Learn how the text chunking size provides different quality retrieval results in a RAG application\n",
    "- Learn how different embeddings models provide different results\n",
    "- Learn how to load an Azure AI Search index for a Vector Store\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "> NOTE: We need to use Semantic Kernel in this notebook in order to work with the embeddings and chunking (those features are not yet in Agent Framework as of the beginning of Jan 2026)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528454ce",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget:Microsoft.Agents.AI.AzureAI, 1.0.0-preview.260108.1\"\n",
    "#r \"nuget:Microsoft.SemanticKernel\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd034f05",
   "metadata": {},
   "source": [
    "## Step 1: Chunk files into smaller pieces\n",
    "\n",
    "### Document Chunking \n",
    "\n",
    "The process of taking a document and splitting into pieces is often referred to as \"chunking\". There are many ways to split a document and it isn't a *one-size-fits-all* activity, so you need to keep in mind how a document needs to be split in order to provide the most valuable chunks for your retrieval system.\n",
    "\n",
    "Important things to remember about these chunks:\n",
    "\n",
    "- We will get embeddings for each chunk\n",
    "- Relevant chunks will be found by a similarity search using embeddings\n",
    "- Often times an overlap of 10 - 20% is used if there is not a clean way to split the document\n",
    "- When working with real documents, you may need to address tables and images (images typically have different embedding models or need to be *verbalized*)\n",
    "- Each chunk needs to fit in the context window of the LLM, and keep in mind things can get lost in the middle when the context is too big\n",
    "- You may need to modify your chunking to improve the retrieval quality of your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a07188",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "using System;\n",
    "using System.Collections.Generic;\n",
    "using System.IO;\n",
    "using System.Linq;\n",
    "using System.Threading.Tasks;\n",
    "using Microsoft.SemanticKernel.Text;\n",
    "\n",
    "#pragma warning disable SKEXP0001, SKEXP0050\n",
    "\n",
    "/// <summary>\n",
    "/// Reads a markdown file and chunks it into smaller pieces using Semantic Kernel's TextChunker.\n",
    "/// </summary>\n",
    "/// <param name=\"filePath\">Path to the markdown file</param>\n",
    "/// <param name=\"maxTokenPerLine\">Maximum number of tokens per line</param>\n",
    "/// <returns>List of text chunks</returns>\n",
    "public static async Task<List<string>> ChunkMarkdownFileAsync(string filePath, int maxTokenPerLine = 256)\n",
    "{\n",
    "    // Step 1: Read the markdown file from the file system\n",
    "    Console.WriteLine($\"Reading file: {filePath}\");\n",
    "    string markdownContent;\n",
    "    \n",
    "    try\n",
    "    {\n",
    "        markdownContent = await File.ReadAllTextAsync(filePath);\n",
    "        Console.WriteLine($\"Successfully read file. Total characters: {markdownContent.Length}\\n\");\n",
    "    }\n",
    "    catch (FileNotFoundException)\n",
    "    {\n",
    "        Console.WriteLine($\"Error: File '{filePath}' not found.\");\n",
    "        return new List<string>();\n",
    "    }\n",
    "    catch (Exception e)\n",
    "    {\n",
    "        Console.WriteLine($\"Error reading file: {e.Message}\");\n",
    "        return new List<string>();\n",
    "    }\n",
    "    \n",
    "    // Step 2: Use Semantic Kernel's TextChunker to split into smaller pieces\n",
    "    Console.WriteLine($\"Chunking text with max_token_per_line={maxTokenPerLine}...\\n\");\n",
    "    \n",
    "    // Split the text into chunks\n",
    "    var chunks = TextChunker.SplitMarkDownLines(\n",
    "        text: markdownContent,\n",
    "        maxTokensPerLine: maxTokenPerLine\n",
    "    );\n",
    "    \n",
    "    // Step 3: Capture all chunks into a list variable\n",
    "    List<string> chunkList = chunks.ToList();\n",
    "    \n",
    "    Console.WriteLine($\"Total chunks created: {chunkList.Count}\\n\");\n",
    "    \n",
    "    // Step 4: Print out the first 3 chunks (or fewer if less than 3 exist)\n",
    "    int chunksToDisplay = Math.Min(3, chunkList.Count);\n",
    "    Console.WriteLine($\"Displaying first {chunksToDisplay} chunks:\\n\");\n",
    "    Console.WriteLine(new string('=', 80));\n",
    "    \n",
    "    for (int i = 0; i < chunksToDisplay; i++)\n",
    "    {\n",
    "        Console.WriteLine($\"\\n--- Chunk {i + 1} ---\");\n",
    "        Console.WriteLine($\"Length: {chunkList[i].Length} characters\");\n",
    "        Console.WriteLine($\"Content:\\n{chunkList[i]}\");\n",
    "        Console.WriteLine(new string('-', 40));\n",
    "    }\n",
    "    \n",
    "    Console.WriteLine(new string('=', 80));\n",
    "    \n",
    "    return chunkList;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20988983",
   "metadata": {},
   "source": [
    "Next, you can now use the above method to split the sample markdown file (in the **/data** folder) into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f07db",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "using System;\n",
    "using System.Collections.Generic;\n",
    "using System.Linq;\n",
    "using System.Threading.Tasks;\n",
    "\n",
    "// Specify the path to your markdown file\n",
    "string markdownFilePath = \"./../../assets/sample.md\";\n",
    "\n",
    "// Chunk the markdown file\n",
    "List<string> chunks = await ChunkMarkdownFileAsync(\n",
    "    filePath: markdownFilePath,\n",
    "    maxTokenPerLine: 256  // Adjust chunk size as needed\n",
    ");\n",
    "\n",
    "if (chunks != null && chunks.Count > 0)\n",
    "{\n",
    "    // Print summary statistics\n",
    "    double avgChunkSize = chunks.Average(chunk => chunk.Length);\n",
    "    int smallestChunkSize = chunks.Min(chunk => chunk.Length);\n",
    "    int largestChunkSize = chunks.Max(chunk => chunk.Length);\n",
    "    \n",
    "    Console.WriteLine(\"\\nChunking Summary:\");\n",
    "    Console.WriteLine($\"  - Total chunks: {chunks.Count}\");\n",
    "    Console.WriteLine($\"  - Average chunk size: {avgChunkSize:F2} characters\");\n",
    "    Console.WriteLine($\"  - Smallest chunk: {smallestChunkSize} characters\");\n",
    "    Console.WriteLine($\"  - Largest chunk: {largestChunkSize} characters\");\n",
    "}\n",
    "\n",
    "// The chunks list is now available for use in the next notebook\n",
    "Console.WriteLine(\"\\n‚úÖ Chunks are now stored in the 'chunks' variable for use in the next step.\");\n",
    "Console.WriteLine(\"   You can access individual chunks with chunks[0], chunks[1], etc.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c438328",
   "metadata": {},
   "source": [
    "### Try Using LangChain's MarkdownHeaderTextSplitter (optional)\n",
    "\n",
    "LangChain is another popular python package used with RAG applications - and there is a nuget package for the [LangChan](https://github.com/tryAGI/LangChain) .NET project. I'm not sure how active development is on it these days. They have many options than Semantic Kernal. In the code below you'll explore the MarkdownHeaderTextSplitter may provide a better splitter option for you.\n",
    "\n",
    "First you'll need to install the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10efecef",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget:LangChain, 0.17.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e50520",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "using System;\n",
    "using System.Collections.Generic;\n",
    "using System.IO;\n",
    "using System.Linq;\n",
    "using System.Threading.Tasks;\n",
    "using LangChain.Splitters.Text;\n",
    "\n",
    "\n",
    "/// <summary>\n",
    "/// Read a markdown file and split it into chunks based on headers.\n",
    "/// </summary>\n",
    "/// <param name=\"filePath\">Path to the markdown file</param>\n",
    "/// <returns>List of document chunks with metadata</returns>\n",
    "public static async Task<List<string>> LangChain_ChunkMarkdownFileAsync(string filePath)\n",
    "{\n",
    "    // Step 1: Read the markdown file\n",
    "    string markdownContent;\n",
    "    try\n",
    "    {\n",
    "        markdownContent = await File.ReadAllTextAsync(filePath);\n",
    "        Console.WriteLine($\"Successfully read file: {filePath}\");\n",
    "        Console.WriteLine($\"File size: {markdownContent.Length} characters\\n\");\n",
    "    }\n",
    "    catch (FileNotFoundException)\n",
    "    {\n",
    "        Console.WriteLine($\"Error: File '{filePath}' not found.\");\n",
    "        return new List<string>();\n",
    "    }\n",
    "    catch (Exception e)\n",
    "    {\n",
    "        Console.WriteLine($\"Error reading file: {e.Message}\");\n",
    "        return new List<string>();\n",
    "    }\n",
    "\n",
    "    // Step 2: Configure the MarkdownHeaderTextSplitter\n",
    "    // Define which headers to split on and their metadata keys\n",
    "    var headersToSplitOn = new string[] { \"#\", \"##\", \"###\" };\n",
    "    \n",
    "    // Create the splitter instance\n",
    "    var markdownSplitter = new MarkdownHeaderTextSplitter(\n",
    "        headersToSplitOn: headersToSplitOn,\n",
    "        includeHeaders: true  // Keep headers in the content\n",
    "    );\n",
    "\n",
    "    // Step 3: Split the document and capture chunks\n",
    "    var chunks = markdownSplitter.SplitText(markdownContent);\n",
    "\n",
    "    // Convert to list of ChunkInfo for easier handling\n",
    "    var chunkList = new List<string>();\n",
    "    for (int i = 0; i < chunks.Count; i++)\n",
    "    {\n",
    "        var chunk = chunks[i];\n",
    "        chunkList.Add(chunk);\n",
    "    }\n",
    "\n",
    "    Console.WriteLine($\"Total number of chunks created: {chunkList.Count}\\n\");\n",
    "    Console.WriteLine(new string('=', 60));\n",
    "\n",
    "    // Step 4: Print the first 3 chunks (or fewer if less than 3 exist)\n",
    "    int chunksToDisplay = Math.Min(3, chunkList.Count);\n",
    "\n",
    "    for (int i = 0; i < chunksToDisplay; i++)\n",
    "    {\n",
    "        var chunk = chunkList[i];\n",
    "        Console.WriteLine($\"\\nüìÑ CHUNK {i + 1}:\");\n",
    "        Console.WriteLine($\"   Length: {chunk.Length} characters\");\n",
    "        Console.WriteLine($\"   Content preview:\");\n",
    "        Console.WriteLine(new string('-', 40));\n",
    "\n",
    "        // Display first 300 characters of content (or full if shorter)\n",
    "        string contentPreview = chunk.Length > 300 \n",
    "            ? chunk.Substring(0, 300) + \"...\"\n",
    "            : chunk;\n",
    "        \n",
    "        Console.WriteLine(contentPreview);\n",
    "        Console.WriteLine(new string('-', 40));\n",
    "    }\n",
    "\n",
    "    // Return the full list of chunks for use in next lab\n",
    "    return chunkList;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc1435",
   "metadata": {},
   "source": [
    "Next, you can now use the above method to split the same sample markdown file (in the **/data** folder) into chunks.\n",
    "\n",
    "> NOTE: the LangChain splitter splits on sections and provided metadata about the section hierarchy (which may be useful for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11bd6f",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "using System;\n",
    "using System.Collections.Generic;\n",
    "using System.Linq;\n",
    "using System.Threading.Tasks;\n",
    "\n",
    "// Specify the path to your markdown file\n",
    "string markdownFilePath = \"./../../assets/sample.md\";\n",
    "\n",
    "// Chunk the markdown file\n",
    "List<string> lcChunks = await LangChain_ChunkMarkdownFileAsync(markdownFilePath);\n",
    "\n",
    "if (lcChunks != null && lcChunks.Any())\n",
    "{\n",
    "    Console.WriteLine(\"\\n\" + new string('=', 60));\n",
    "    Console.WriteLine(\"üìä SUMMARY STATISTICS:\");\n",
    "    Console.WriteLine($\"   Total chunks: {lcChunks.Count}\");\n",
    "    \n",
    "    int totalChars = lcChunks.Sum(chunk => chunk.Length);\n",
    "    double avgChunkSize = lcChunks.Count > 0 ? (double)totalChars / lcChunks.Count : 0;\n",
    "    Console.WriteLine($\"   Average chunk size: {avgChunkSize:F1} characters\");\n",
    "    \n",
    "    var maxChunk = lcChunks.OrderByDescending(x => x.Length).First();\n",
    "    var minChunk = lcChunks.OrderBy(x => x.Length).First();\n",
    "    Console.WriteLine($\"   Largest chunk: {maxChunk.Length} characters (chunk #{maxChunk.Index})\");\n",
    "    Console.WriteLine($\"   Smallest chunk: {minChunk.Length} characters (chunk #{minChunk.Index})\");\n",
    "}\n",
    "\n",
    "// The chunks list is now available for use in the next notebook\n",
    "Console.WriteLine($\"\\n‚úÖ Chunks are now stored in the 'chunks' variable for use in the next step.\");\n",
    "Console.WriteLine($\"   You can access individual chunks with chunks[0], chunks[1], etc.\");\n",
    "\n",
    "// If going to use the LangChain chunks in the next step, set the chunks variable to the chunk text\n",
    "List<string> chunks = lcChunks.Select(item => item).ToList();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de24fd5",
   "metadata": {},
   "source": [
    "## Step 2: Create Embeddings for Semantic Searches\n",
    "\n",
    "In this step you'll use AzureOpenAI to create the embeddings for the chunks you created above - you'll need to decide with chunking technique you like best.\n",
    "\n",
    "The code below will utilize the older text-embedding-ada-002 model for creating the embeddings. In Step 4, you'll get to compare the embeddings from OpenAI how they differ in a semantic search.\n",
    "\n",
    "First you'll need to install the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22715e",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget:Azure.AI.OpenAI, *-*\"\n",
    "#r \"nuget:Azure.Core, *-*\"\n",
    "#r \"nuget:DotNetEnv, *-*\"\n",
    "#r \"nuget:Microsoft.Extensions.Configuration, 10.0.1\"\n",
    "#r \"nuget:Microsoft.Extensions.Configuration.Json, 10.0.1\"\n",
    "#r \"nuget:Microsoft.Extensions.configuration.Binder, 10.0.1\"\n",
    "#r \"nuget:Microsoft.Extensions.Configuration.EnvironmentVariables, 10.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ad1d06",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "public class AzureOpenAISettings\n",
    "{\n",
    "    public string ApiKey { get; set; }\n",
    "    public string Endpoint { get; set; }\n",
    "    public string ApiVersion { get; set; }\n",
    "    public string EmbeddingDeploymentName { get; set; }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96ea18",
   "metadata": {},
   "source": [
    "Load the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "public const string DefaultConfigFileName = \"appsettings.Local.json\";\n",
    "\n",
    "public static string? FindConfigDirectory(string fileName)\n",
    "{\n",
    "    var directory = new DirectoryInfo(Directory.GetCurrentDirectory());\n",
    "\n",
    "    while (directory is not null)\n",
    "    {\n",
    "        if (File.Exists(Path.Combine(directory.FullName, fileName)))\n",
    "        {\n",
    "            return directory.FullName;\n",
    "        }\n",
    "        directory = directory.Parent;\n",
    "    }\n",
    "\n",
    "    return null;\n",
    "}\n",
    "\n",
    "var basePath = FindConfigDirectory(DefaultConfigFileName)\n",
    "            ?? throw new InvalidOperationException(\n",
    "                $\"Could not find {DefaultConfigFileName} in current directory or any parent directory.\");\n",
    "\n",
    "// Load configuration from appsettings.json\n",
    "var configuration = new ConfigurationBuilder()\n",
    "    .SetBasePath(basePath)\n",
    "    .AddJsonFile(\"appsettings.Local.json\", optional: true, reloadOnChange: true) // Optional environment-specific settings\n",
    "    .AddEnvironmentVariables()\n",
    "    .Build();\n",
    "\n",
    "// Bind configuration to settings object\n",
    "var _settings = new AzureOpenAISettings()\n",
    "{\n",
    "    Endpoint = configuration[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    ApiVersion = configuration[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    EmbeddingDeploymentName = configuration[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"]\n",
    "};        \n",
    "\n",
    "foreach (var kvp in configuration.AsEnumerable())\n",
    "{\n",
    "    if (!string.IsNullOrEmpty(kvp.Value))\n",
    "    {\n",
    "        Environment.SetEnvironmentVariable(kvp.Key, kvp.Value);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e14235",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "using System;\n",
    "using System.Collections.Generic;\n",
    "using System.IO;\n",
    "using System.Linq;\n",
    "using System.Threading.Tasks;\n",
    "using Microsoft.Extensions.Configuration;\n",
    "using Azure;\n",
    "using Azure.AI.OpenAI;\n",
    "using Azure.Identity;\n",
    "\n",
    "public class EmbeddingService\n",
    "{\n",
    "    private readonly AzureOpenAIClient _client;\n",
    "    private readonly AzureOpenAISettings _settings;\n",
    "    \n",
    "    public EmbeddingService()\n",
    "    {\n",
    "           // Bind configuration to settings object\n",
    "        _settings = new AzureOpenAISettings()\n",
    "        {\n",
    "            Endpoint = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            ApiVersion = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\"),\n",
    "            EmbeddingDeploymentName = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "        };        \n",
    "        \n",
    "        Console.WriteLine(\"Azure OpenAI Settings:\");\n",
    "        Console.WriteLine($\"  - Endpoint: {_settings.Endpoint}\");\n",
    "        Console.WriteLine($\"  - API Version: {_settings.ApiVersion}\");\n",
    "        Console.WriteLine($\"  - Embedding Deployment Name: {_settings.EmbeddingDeploymentName}\");\n",
    "        \n",
    "        // Initialize Azure OpenAI client\n",
    "        _client = new AzureOpenAIClient(\n",
    "            new Uri(_settings.Endpoint),\n",
    "            new DefaultAzureCredential()\n",
    "        );\n",
    "    }\n",
    "    \n",
    "    public async Task<List<float[]>> EmbedChunksAsync(List<string> textChunks, string model = null)\n",
    "    {\n",
    "        // Use provided model or default from settings\n",
    "        string deploymentName = model ?? _settings.EmbeddingDeploymentName;\n",
    "        \n",
    "        var embeddingClient = _client.GetEmbeddingClient(deploymentName);\n",
    "        \n",
    "        // Create embeddings for all chunks\n",
    "        var response = await embeddingClient.GenerateEmbeddingsAsync(textChunks);\n",
    "        \n",
    "        // Extract embeddings from response\n",
    "        return response.Value.Select(item => item.ToFloats().ToArray()).ToList();\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82340fb4",
   "metadata": {},
   "source": [
    "Next you use the above utility to create embeddings of the chunks (created earlier) and take a look at a few of the returned vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c3b5f",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "var embeddings = await new EmbeddingService().EmbedChunksAsync(chunks);\n",
    "\n",
    "// Single embedding (first item)\n",
    "Console.WriteLine(\"First embedding:\");\n",
    "Console.WriteLine(embeddings[0]); //  a List[float]\n",
    "Console.WriteLine(embeddings[0].Length + \" dimensions\");\n",
    "\n",
    "// First two embeddings\n",
    "Console.WriteLine(\"\\nFirst two embeddings:\");\n",
    "for (int i = 0; i < 2 && i < embeddings.Count; i++)  // slice to first 2\n",
    "{\n",
    "    Console.WriteLine($\"Embedding {i}:\");\n",
    "    Console.WriteLine(string.Join(\", \", embeddings[i].Take(8)) + \" ...\");  // show just first few dims to keep output short\n",
    "    Console.WriteLine(\"dim: \" + embeddings[i].Length);\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab45ac6d",
   "metadata": {},
   "source": [
    "## Step 3: Load Azure AI Search Index\n",
    "\n",
    "Next step is the inserting of the chunks and embeddings into a vector database. In this step we'll use Azure AI Search as the vector database.\n",
    "\n",
    "First you'll need to install the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822ce02",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget:Azure.Search.Documents, *-*\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9516e709",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "using System;\n",
    "using System.Collections.Generic;\n",
    "using System.Linq;\n",
    "using System.Threading.Tasks;\n",
    "using Azure;\n",
    "using Azure.Search.Documents;\n",
    "using Azure.Search.Documents.Indexes;\n",
    "using Azure.Search.Documents.Indexes.Models;\n",
    "using Azure.Search.Documents.Models;\n",
    "using Microsoft.Extensions.Configuration;\n",
    "\n",
    "public class AzureSearchSettings\n",
    "{\n",
    "    public string Endpoint { get; set; }\n",
    "    public string ApiKey { get; set; }\n",
    "}\n",
    "\n",
    "public class AzureSearchService\n",
    "{\n",
    "    private readonly string _searchEndpoint;\n",
    "    private readonly string _searchKey;\n",
    "    private readonly string _myInitials;\n",
    "    private readonly string _indexName;\n",
    "    private readonly int _embeddingDimension = 1536; // e.g. 1536 for ada-002\n",
    "    private readonly SearchClient _searchClient;\n",
    "    private readonly SearchIndexClient _indexClient;\n",
    "\n",
    "    // TODO: modify to match config used above\n",
    "    public AzureSearchService()\n",
    "    {\n",
    "        \n",
    "        // Bind configuration to settings object\n",
    "        var settings = new AzureSearchSettings()\n",
    "        {\n",
    "            Endpoint = Environment.GetEnvironmentVariable(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "            ApiKey = Environment.GetEnvironmentVariable(\"AZURE_SEARCH_API_KEY\")\n",
    "        };\n",
    "\n",
    "        _searchEndpoint = settings.Endpoint;\n",
    "        _searchKey = settings.ApiKey;\n",
    "        _myInitials = Environment.GetEnvironmentVariable(\"MY_INITIALS\");\n",
    "        _indexName = $\"{_myInitials?.ToLower()}vectorindex\";\n",
    "\n",
    "        // Create SearchClient for later use\n",
    "        _searchClient = new SearchClient(\n",
    "            new Uri(_searchEndpoint),\n",
    "            _indexName,\n",
    "            new AzureKeyCredential(_searchKey)\n",
    "        );\n",
    "\n",
    "        // Create SearchIndexClient for index management\n",
    "        _indexClient = new SearchIndexClient(\n",
    "            new Uri(_searchEndpoint),\n",
    "            new AzureKeyCredential(_searchKey)\n",
    "        );\n",
    "    }\n",
    "\n",
    "    /// <summary>\n",
    "    /// Ensure an Azure AI Search index exists for chunk text + embeddings.\n",
    "    /// If it does not exist, create it. If it exists, do nothing.\n",
    "    /// </summary>\n",
    "    public async Task EnsureChunkVectorIndexAsync()\n",
    "    {\n",
    "        if (string.IsNullOrEmpty(_myInitials))\n",
    "        {\n",
    "            throw new InvalidOperationException(\"MY_INITIALS configuration must be set to prevent index name collisions.\");\n",
    "        }\n",
    "\n",
    "        if (string.IsNullOrEmpty(_searchEndpoint) || string.IsNullOrEmpty(_searchKey))\n",
    "        {\n",
    "            throw new InvalidOperationException(\"AZURE_SEARCH_ENDPOINT and AZURE_SEARCH_API_KEY must be set.\");\n",
    "        }\n",
    "\n",
    "        // Check if the index already exists\n",
    "        var existingIndexes = _indexClient.GetIndexNamesAsync();\n",
    "        await foreach (var existingName in existingIndexes)\n",
    "        {\n",
    "            if (existingName == _indexName)\n",
    "            {\n",
    "                Console.WriteLine($\"Index '{_indexName}' already exists; skipping creation.\");\n",
    "                return;\n",
    "            }\n",
    "        }\n",
    "\n",
    "        Console.WriteLine($\"Index '{_indexName}' does not exist; creating now...\");\n",
    "\n",
    "        var fields = new List<SearchField>\n",
    "        {\n",
    "            new SimpleField(\"id\", SearchFieldDataType.String)\n",
    "            {\n",
    "                IsKey = true,\n",
    "                IsFilterable = true,\n",
    "                IsSortable = true,\n",
    "                IsFacetable = true\n",
    "            },\n",
    "            new SearchableField(\"content\")\n",
    "            {\n",
    "                AnalyzerName = LexicalAnalyzerName.StandardLucene\n",
    "            },\n",
    "            new SearchField(\"contentVector\", SearchFieldDataType.Collection(SearchFieldDataType.Single))\n",
    "            {\n",
    "                IsSearchable = true,\n",
    "                VectorSearchDimensions = _embeddingDimension,\n",
    "                VectorSearchProfileName = \"chunk-vector-profile\"\n",
    "            }\n",
    "        };\n",
    "\n",
    "        var vectorSearch = new VectorSearch\n",
    "        {\n",
    "            Algorithms =\n",
    "            {\n",
    "                new HnswAlgorithmConfiguration(\"chunk-hnsw-config\")\n",
    "            },\n",
    "            Profiles =\n",
    "            {\n",
    "                new VectorSearchProfile(\"chunk-vector-profile\", \"chunk-hnsw-config\")\n",
    "            }\n",
    "        };\n",
    "\n",
    "        var index = new SearchIndex(_indexName)\n",
    "        {\n",
    "            Fields = fields,\n",
    "            VectorSearch = vectorSearch\n",
    "        };\n",
    "\n",
    "        var result = await _indexClient.CreateIndexAsync(index);\n",
    "        Console.WriteLine($\"Index '{result.Value.Name}' created.\");\n",
    "    }\n",
    "\n",
    "    /// <summary>\n",
    "    /// Upload chunks and their corresponding embeddings to the Azure AI Search index.\n",
    "    /// </summary>\n",
    "    public async Task UploadChunksWithEmbeddingsAsync(\n",
    "        List<string> chunks,\n",
    "        List<float[]> embeddings)\n",
    "    {\n",
    "        if (chunks.Count != embeddings.Count)\n",
    "        {\n",
    "            throw new ArgumentException(\"chunks and embeddings must have the same length\");\n",
    "        }\n",
    "\n",
    "        var documents = new List<SearchDocument>();\n",
    "        \n",
    "        for (int i = 0; i < chunks.Count; i++)\n",
    "        {\n",
    "            var doc = new SearchDocument\n",
    "            {\n",
    "                [\"id\"] = i.ToString(),\n",
    "                [\"content\"] = chunks[i],\n",
    "                [\"contentVector\"] = embeddings[i]\n",
    "            };\n",
    "            documents.Add(doc);\n",
    "        }\n",
    "\n",
    "        // Azure AI Search supports up to 1,000 docs per batch\n",
    "        const int batchSize = 1000;\n",
    "        \n",
    "        for (int start = 0; start < documents.Count; start += batchSize)\n",
    "        {\n",
    "            var batch = documents.Skip(start).Take(batchSize).ToList();\n",
    "            \n",
    "            var indexDocumentsBatch = IndexDocumentsBatch.MergeOrUpload(batch);\n",
    "            var result = await _searchClient.IndexDocumentsAsync(indexDocumentsBatch);\n",
    "            \n",
    "            // Check status per doc\n",
    "            int succeeded = result.Value.Results.Count(r => r.Succeeded);\n",
    "            Console.WriteLine($\"Uploaded {succeeded}/{batch.Count} documents in batch starting at {start}.\");\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3693a95",
   "metadata": {},
   "source": [
    "Next you can run the code that will ensure the index has been created and then load the chunks and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b0f90",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "var searchService = new AzureSearchService();\n",
    "\n",
    "// Ensure index exists\n",
    "await searchService.EnsureChunkVectorIndexAsync();\n",
    "\n",
    "// Upload chunks with embeddings\n",
    "await searchService.UploadChunksWithEmbeddingsAsync(chunks, embeddings);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68738257",
   "metadata": {},
   "source": [
    "Next let's see what the semantic search results would be for these questions:\n",
    "- ‚ÄúExplain the difference between supervised, unsupervised, and reinforcement learning.‚Äù\n",
    "- ‚ÄúWhat kinds of real‚Äëworld problems can machine learning solve today?‚Äù\n",
    "- ‚ÄúHow does reinforcement learning decide which actions to take to maximize rewards?‚Äù\n",
    "- ‚ÄúGive some examples of how machine learning is used in healthcare and fraud prevention.‚Äù\n",
    "- ‚ÄúWhy is machine learning becoming more important as the amount of data grows?‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3960a9e",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "using System.Collections.Generic;\n",
    "using System.Linq;\n",
    "using System.Threading.Tasks;\n",
    "using Azure;\n",
    "using Azure.AI.OpenAI;\n",
    "using Azure.Search.Documents;\n",
    "using Azure.Search.Documents.Models;\n",
    "using Microsoft.Extensions.Configuration;\n",
    "\n",
    "public class SearchQueryService\n",
    "{\n",
    "    private readonly AzureOpenAIClient _aoaiClient;\n",
    "    private readonly SearchClient _searchClient;\n",
    "    private readonly string _embeddingDeploymentName;\n",
    "    \n",
    "    public SearchQueryService()\n",
    "    {\n",
    "        // Bind configuration to settings object\n",
    "        var settings = new AzureOpenAISettings()\n",
    "        {\n",
    "            Endpoint = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            ApiVersion = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\"),\n",
    "            EmbeddingDeploymentName = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "        };        \n",
    "            \n",
    "        // Initialize Azure OpenAI client\n",
    "        _embeddingDeploymentName = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\") ?? \"text-embedding-ada-002\";\n",
    "        \n",
    "        _aoaiClient = new AzureOpenAIClient(\n",
    "            new Uri(settings.Endpoint),\n",
    "            new DefaultAzureCredential()\n",
    "        );\n",
    "        \n",
    "        // Initialize Search client\n",
    "        var searchEndpoint = Environment.GetEnvironmentVariable(\"AZURE_SEARCH_ENDPOINT\");\n",
    "        var searchKey = Environment.GetEnvironmentVariable(\"AZURE_SEARCH_API_KEY\");\n",
    "        var myInitials = Environment.GetEnvironmentVariable(\"MY_INITIALS\");\n",
    "        var indexName = $\"{myInitials?.ToLower()}vectorindex\";\n",
    "        \n",
    "        _searchClient = new SearchClient(\n",
    "            new Uri(searchEndpoint),\n",
    "            indexName,\n",
    "            new AzureKeyCredential(searchKey)\n",
    "        );\n",
    "    }\n",
    "    \n",
    "    /// <summary>\n",
    "    /// Create a single embedding vector for a query string using Azure OpenAI.\n",
    "    /// </summary>\n",
    "    public async Task<float[]> EmbedQueryAsync(string text, string model = null)\n",
    "    {\n",
    "        var deploymentName = model ?? _embeddingDeploymentName;\n",
    "        var embeddingClient = _aoaiClient.GetEmbeddingClient(deploymentName);\n",
    "        \n",
    "        var response = await embeddingClient.GenerateEmbeddingsAsync(new List<string> { text });\n",
    "        return response.Value.First().ToFloats().ToArray();\n",
    "    }\n",
    "    \n",
    "    /// <summary>\n",
    "    /// Run test queries against the search index.\n",
    "    /// </summary>\n",
    "    public async Task RunTestQueriesAsync(List<string> queries, bool useHybrid = true, int topK = 3)\n",
    "    {\n",
    "        foreach (var query in queries)\n",
    "        {\n",
    "            Console.WriteLine(new string('=', 80));\n",
    "            Console.WriteLine($\"Query: {query}\");\n",
    "            Console.WriteLine($\"Hybrid search: {useHybrid}\");\n",
    "            Console.WriteLine(new string('-', 80));\n",
    "            \n",
    "            // Generate embedding for the query\n",
    "            var queryVector = await EmbedQueryAsync(query);\n",
    "            \n",
    "            // Create vector query\n",
    "            var vectorQuery = new VectorizedQuery(queryVector)\n",
    "            {\n",
    "                Fields = { \"contentVector\" }\n",
    "            };\n",
    "            \n",
    "            // Configure search options\n",
    "            var searchOptions = new SearchOptions\n",
    "            {\n",
    "                VectorSearch = new VectorSearchOptions\n",
    "                {\n",
    "                    Queries = { vectorQuery }\n",
    "                },\n",
    "                Size = topK\n",
    "            };\n",
    "            \n",
    "            // Perform search\n",
    "            SearchResults<SearchDocument> results;\n",
    "            \n",
    "            if (useHybrid)\n",
    "            {\n",
    "                // Hybrid search: both text and vector\n",
    "                results = await _searchClient.SearchAsync<SearchDocument>(\n",
    "                    searchText: query,\n",
    "                    searchOptions);\n",
    "            }\n",
    "            else\n",
    "            {\n",
    "                // Pure vector search\n",
    "                results = await _searchClient.SearchAsync<SearchDocument>(\n",
    "                    searchText: null,\n",
    "                    searchOptions);\n",
    "            }\n",
    "            \n",
    "            // Display results\n",
    "            int i = 0;\n",
    "            await foreach (var result in results.GetResultsAsync())\n",
    "            {\n",
    "                var doc = result.Document;\n",
    "                var score = result.Score;\n",
    "                \n",
    "                if (score.HasValue)\n",
    "                {\n",
    "                    Console.WriteLine($\"[{i}] id={doc[\"id\"]}  score={score.Value:F4}\");\n",
    "                }\n",
    "                else\n",
    "                {\n",
    "                    Console.WriteLine($\"[{i}] id={doc[\"id\"]}\");\n",
    "                }\n",
    "                \n",
    "                Console.WriteLine(doc[\"content\"]);\n",
    "                Console.WriteLine(new string('-', 40));\n",
    "                i++;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5779a8",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "var testQueries = new List<string>\n",
    "{\n",
    "    \"Explain the difference between supervised, unsupervised, and reinforcement learning.\",\n",
    "    //\"What kinds of real-world problems can machine learning solve today?\",\n",
    "    //\"How does reinforcement learning decide which actions to take to maximize rewards?\",\n",
    "    //\"Give some examples of how machine learning is used in healthcare and fraud prevention.\",\n",
    "    //\"Why is machine learning becoming more important as the amount of data grows?\",\n",
    "};\n",
    "\n",
    "var queryService = new SearchQueryService();\n",
    "\n",
    "// Hybrid on:\n",
    "await queryService.RunTestQueriesAsync(testQueries, useHybrid: true, topK: 3);\n",
    "\n",
    "// Hybrid off:\n",
    "//await queryService.RunTestQueriesAsync(testQueries, useHybrid: false, topK: 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc96ab4",
   "metadata": {},
   "source": [
    "## Step 4: Test Different Chunk Sizes and Embedding Models\n",
    "\n",
    "In this step, you get to explore the differences between using:\n",
    "- text-embedding-ada-002\n",
    "- text-embedding-3-small\n",
    "- text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fd5d59",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "using Azure;\n",
    "using Azure.AI.OpenAI;\n",
    "using Azure.Search.Documents;\n",
    "using Azure.Search.Documents.Models;\n",
    "using System;\n",
    "using System.Collections.Generic;\n",
    "using System.Linq;\n",
    "using System.Threading.Tasks;\n",
    "\n",
    "/// <summary>\n",
    "/// Compare semantic search results across different embedding model indexes.\n",
    "/// </summary>\n",
    "public class AzureSearchComparison\n",
    "{\n",
    "    private readonly string _openai_endpoint;\n",
    "    private readonly string _openai_api_key;\n",
    "    private readonly string _search_endpoint;\n",
    "    private readonly AzureKeyCredential _search_credential;\n",
    "    private readonly Dictionary<string, IndexInfo> _indexes;\n",
    "\n",
    "    public class IndexInfo\n",
    "    {\n",
    "        public int Dimensions { get; set; }\n",
    "        public string Model { get; set; } = string.Empty;\n",
    "    }\n",
    "\n",
    "    public class SearchResultItem\n",
    "    {\n",
    "        public string Id { get; set; } = string.Empty;\n",
    "        public string Content { get; set; } = string.Empty;\n",
    "        public double Score { get; set; }\n",
    "    }\n",
    "\n",
    "    /// <summary>\n",
    "    /// Initialize Azure Search connection.\n",
    "    /// </summary>\n",
    "    public AzureSearchComparison()\n",
    "    {\n",
    "        \n",
    "        _search_endpoint = Environment.GetEnvironmentVariable(\"AZURE_SEARCH_ENDPOINT\");\n",
    "        _search_credential = new AzureKeyCredential(Environment.GetEnvironmentVariable(\"AZURE_SEARCH_API_KEY\"));\n",
    "        _openai_endpoint = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_ENDPOINT\");\n",
    "\n",
    "        // Define the indexes and their embedding dimensions\n",
    "        _indexes = new Dictionary<string, IndexInfo>\n",
    "        {\n",
    "            [\"large3index\"] = new IndexInfo\n",
    "            {\n",
    "                Dimensions = 3072,  // Dimension for text-embedding-3-large\n",
    "                Model = \"text-embedding-3-large\"\n",
    "            },\n",
    "            [\"small3index\"] = new IndexInfo\n",
    "            {\n",
    "                Dimensions = 1536,  // Dimension for text-embedding-3-small\n",
    "                Model = \"text-embedding-3-small\"\n",
    "            },\n",
    "            [\"ada002index\"] = new IndexInfo\n",
    "            {\n",
    "                Dimensions = 1536,\n",
    "                Model = \"text-embedding-ada-002\"\n",
    "            }\n",
    "        };\n",
    "    }\n",
    "\n",
    "    /// <summary>\n",
    "    /// Generate embedding for the query text using specified model.\n",
    "    /// </summary>\n",
    "    /// <param name=\"text\">Query text to embed</param>\n",
    "    /// <param name=\"model\">OpenAI embedding model name</param>\n",
    "    /// <returns>List of floats representing the embedding vector</returns>\n",
    "    private async Task<ReadOnlyMemory<float>> GetEmbeddingAsync(string text, string model)\n",
    "    {\n",
    "        var client = new AzureOpenAIClient(\n",
    "            new Uri(_openai_endpoint),\n",
    "            new DefaultAzureCredential()\n",
    "        );\n",
    "\n",
    "        var embeddingClient = client.GetEmbeddingClient(model);\n",
    "        var response = await embeddingClient.GenerateEmbeddingAsync(text);\n",
    "\n",
    "        return response.Value.ToFloats();\n",
    "    }\n",
    "\n",
    "    /// <summary>\n",
    "    /// Search a specific Azure AI Search index using vector similarity.\n",
    "    /// </summary>\n",
    "    /// <param name=\"indexName\">Name of the Azure search index</param>\n",
    "    /// <param name=\"queryText\">Text query to search for</param>\n",
    "    /// <param name=\"vectorDimensions\">Dimension of the embedding vectors</param>\n",
    "    /// <param name=\"embeddingModel\">OpenAI model to use for generating query embedding</param>\n",
    "    /// <param name=\"topK\">Number of top results to return</param>\n",
    "    /// <returns>List of search results with scores</returns>\n",
    "    public async Task<List<SearchResultItem>> SearchIndexAsync(\n",
    "        string indexName,\n",
    "        string queryText,\n",
    "        int vectorDimensions,\n",
    "        string embeddingModel,\n",
    "        int topK = 5)\n",
    "    {\n",
    "        // Create search client for this index\n",
    "        var searchClient = new SearchClient(\n",
    "            new Uri(_search_endpoint),\n",
    "            indexName,\n",
    "            _search_credential\n",
    "        );\n",
    "\n",
    "        // Generate embedding for the query\n",
    "        Console.WriteLine($\"  Generating embedding with {embeddingModel}...\");\n",
    "        var queryVector = await GetEmbeddingAsync(queryText, embeddingModel);\n",
    "\n",
    "        // Create vector query\n",
    "        var vectorQuery = new VectorizedQuery(queryVector)\n",
    "        {\n",
    "            KNearestNeighborsCount = topK,\n",
    "            Fields = { \"contentVector\" }\n",
    "        };\n",
    "\n",
    "        // Perform search\n",
    "        var searchOptions = new SearchOptions\n",
    "        {\n",
    "            Size = topK,\n",
    "            Select = { \"id\", \"content\" },\n",
    "            VectorSearch = new VectorSearchOptions\n",
    "            {\n",
    "                Queries = { vectorQuery }\n",
    "            }\n",
    "        };\n",
    "\n",
    "        var response = await searchClient.SearchAsync<SearchDocument>(null, searchOptions);\n",
    "\n",
    "        // Collect results\n",
    "        var searchResults = new List<SearchResultItem>();\n",
    "        await foreach (var result in response.Value.GetResultsAsync())\n",
    "        {\n",
    "            var content = result.Document[\"content\"]?.ToString() ?? string.Empty;\n",
    "            searchResults.Add(new SearchResultItem\n",
    "            {\n",
    "                Id = result.Document[\"id\"]?.ToString() ?? string.Empty,\n",
    "                Content = content.Length > 200 ? content[..200] + \"...\" : content,\n",
    "                Score = result.Score ?? 0\n",
    "            });\n",
    "        }\n",
    "\n",
    "        return searchResults;\n",
    "    }\n",
    "\n",
    "    /// <summary>\n",
    "    /// Compare search results across all three indexes.\n",
    "    /// </summary>\n",
    "    /// <param name=\"queryText\">The search query</param>\n",
    "    /// <param name=\"topK\">Number of top results to show per index</param>\n",
    "    public async Task<Dictionary<string, List<SearchResultItem>>> CompareSearchResultsAsync(\n",
    "        string queryText,\n",
    "        int topK = 3)\n",
    "    {\n",
    "        Console.WriteLine();\n",
    "        Console.WriteLine(new string('=', 80));\n",
    "        Console.WriteLine(\"üîç SEMANTIC SEARCH COMPARISON\");\n",
    "        Console.WriteLine(new string('=', 80));\n",
    "        Console.WriteLine($\"\\nQuery: '{queryText}'\\n\");\n",
    "        Console.WriteLine($\"Retrieving top {topK} results from each index...\\n\");\n",
    "\n",
    "        var allResults = new Dictionary<string, List<SearchResultItem>>();\n",
    "\n",
    "        // Search each index\n",
    "        foreach (var (indexName, indexInfo) in _indexes)\n",
    "        {\n",
    "            Console.WriteLine($\"\\nüìä Searching {indexName.ToUpper()}\");\n",
    "            Console.WriteLine($\"   Model: {indexInfo.Model}\");\n",
    "            Console.WriteLine($\"   Dimensions: {indexInfo.Dimensions}\");\n",
    "            Console.WriteLine(new string('-', 60));\n",
    "\n",
    "            try\n",
    "            {\n",
    "                var results = await SearchIndexAsync(\n",
    "                    indexName: indexName,\n",
    "                    queryText: queryText,\n",
    "                    vectorDimensions: indexInfo.Dimensions,\n",
    "                    embeddingModel: indexInfo.Model,\n",
    "                    topK: topK\n",
    "                );\n",
    "\n",
    "                allResults[indexName] = results;\n",
    "\n",
    "                // Display results for this index\n",
    "                for (int i = 0; i < results.Count; i++)\n",
    "                {\n",
    "                    var result = results[i];\n",
    "                    Console.WriteLine($\"\\n   Result {i + 1} (Score: {result.Score:F4}):\");\n",
    "                    Console.WriteLine($\"   ID: {result.Id}\");\n",
    "                    Console.WriteLine($\"   Content: {result.Content}\");\n",
    "                }\n",
    "            }\n",
    "            catch (Exception ex)\n",
    "            {\n",
    "                Console.WriteLine($\"   ‚ùå Error searching {indexName}: {ex.Message}\");\n",
    "                allResults[indexName] = new List<SearchResultItem>();\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // Compare and analyze differences\n",
    "        AnalyzeDifferences(allResults, queryText);\n",
    "\n",
    "        return allResults;\n",
    "    }\n",
    "\n",
    "    /// <summary>\n",
    "    /// Analyze and highlight differences between search results.\n",
    "    /// </summary>\n",
    "    /// <param name=\"allResults\">Dictionary of results from each index</param>\n",
    "    /// <param name=\"queryText\">Original query text</param>\n",
    "    private void AnalyzeDifferences(\n",
    "        Dictionary<string, List<SearchResultItem>> allResults,\n",
    "        string queryText)\n",
    "    {\n",
    "        Console.WriteLine();\n",
    "        Console.WriteLine(new string('=', 80));\n",
    "        Console.WriteLine(\"üìà ANALYSIS: Differences Between Embedding Models\");\n",
    "        Console.WriteLine(new string('=', 80));\n",
    "\n",
    "        // Check if all indexes returned results\n",
    "        var indexesWithResults = allResults\n",
    "            .Where(kvp => kvp.Value.Count > 0)\n",
    "            .Select(kvp => kvp.Key)\n",
    "            .ToList();\n",
    "\n",
    "        if (indexesWithResults.Count < 2)\n",
    "        {\n",
    "            Console.WriteLine(\"\\n‚ö†Ô∏è  Not enough results to compare. Check your indexes and API keys.\");\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        // Compare top results\n",
    "        Console.WriteLine(\"\\nüéØ Top Result Comparison:\");\n",
    "        Console.WriteLine(new string('-', 40));\n",
    "\n",
    "        foreach (var (indexName, results) in allResults)\n",
    "        {\n",
    "            if (results.Count > 0)\n",
    "            {\n",
    "                var topResult = results[0];\n",
    "                Console.WriteLine($\"\\n{indexName}:\");\n",
    "                Console.WriteLine($\"  Top match ID: {topResult.Id}\");\n",
    "                Console.WriteLine($\"  Score: {topResult.Score:F4}\");\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // Check for agreement on top result\n",
    "        var topIds = allResults\n",
    "            .Where(kvp => kvp.Value.Count > 0)\n",
    "            .Select(kvp => kvp.Value[0].Id)\n",
    "            .ToList();\n",
    "\n",
    "        if (topIds.Distinct().Count() == 1)\n",
    "        {\n",
    "            Console.WriteLine(\"\\n‚úÖ All models agree on the top result!\");\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "            Console.WriteLine(\"\\nüîÑ Models returned different top results\");\n",
    "        }\n",
    "\n",
    "        // Calculate overlap in results\n",
    "        Console.WriteLine(\"\\nüìä Result Overlap Analysis:\");\n",
    "        Console.WriteLine(new string('-', 40));\n",
    "\n",
    "        // Get all unique IDs per index\n",
    "        var resultsList = allResults.ToList();\n",
    "        for (int i = 0; i < resultsList.Count; i++)\n",
    "        {\n",
    "            var (idx1, results1) = resultsList[i];\n",
    "            if (results1.Count == 0) continue;\n",
    "\n",
    "            var ids1 = results1.Select(r => r.Id).ToHashSet();\n",
    "\n",
    "            for (int j = i + 1; j < resultsList.Count; j++)\n",
    "            {\n",
    "                var (idx2, results2) = resultsList[j];\n",
    "                if (results2.Count == 0) continue;\n",
    "\n",
    "                var ids2 = results2.Select(r => r.Id).ToHashSet();\n",
    "\n",
    "                var overlap = ids1.Intersect(ids2).ToList();\n",
    "                var maxCount = Math.Max(ids1.Count, ids2.Count);\n",
    "                var overlapPct = (overlap.Count / (double)maxCount) * 100;\n",
    "\n",
    "                Console.WriteLine($\"\\n{idx1} vs {idx2}:\");\n",
    "                Console.WriteLine($\"  Overlapping results: {overlap.Count}/{maxCount}\");\n",
    "                Console.WriteLine($\"  Similarity: {overlapPct:F1}%\");\n",
    "\n",
    "                if (overlap.Count > 0)\n",
    "                {\n",
    "                    Console.WriteLine($\"  Common IDs: {string.Join(\", \", overlap.OrderBy(x => x))}\");\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee4293",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "// Initialize the comparison tool\n",
    "var searcher = new AzureSearchComparison();\n",
    "\n",
    "// The search query\n",
    "var query = \"Explain the difference between supervised, unsupervised, and reinforcement learning.\";\n",
    "\n",
    "// Run the comparison\n",
    "Console.WriteLine(\"\\nüöÄ Starting semantic search comparison across embedding models...\");\n",
    "var results = await searcher.CompareSearchResultsAsync(query, topK: 3);\n",
    "\n",
    "// Additional insights\n",
    "Console.WriteLine();\n",
    "Console.WriteLine(new string('=', 80));\n",
    "Console.WriteLine(\"üí° KEY INSIGHTS FOR STUDENTS:\");\n",
    "Console.WriteLine(new string('=', 80));\n",
    "Console.WriteLine(\"\"\"\n",
    "\n",
    "1. DIMENSIONALITY: \n",
    "    - text-embedding-3-large (3072 dims) captures more nuanced relationships\n",
    "    - text-embedding-ada-002 and text-embedding-3-small (1536 dims) are more efficient\n",
    "\n",
    "2. PERFORMANCE VS COST:\n",
    "    - Larger models may provide better semantic understanding\n",
    "    - Smaller models are faster and cheaper to run at scale\n",
    "\n",
    "3. USE CASE CONSIDERATIONS:\n",
    "    - For high-precision tasks: Consider larger embedding models\n",
    "    - For high-throughput applications: Smaller models may be sufficient\n",
    "    - Always test with your specific data and queries\n",
    "\n",
    "4. WHAT TO LOOK FOR:\n",
    "    - Do all models find the same top result?\n",
    "    - How much overlap is there in the top 3 results?\n",
    "    - Are the relevance scores significantly different?\n",
    "\"\"\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
