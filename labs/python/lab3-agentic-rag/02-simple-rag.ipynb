{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d04f9e6",
   "metadata": {},
   "source": [
    "# Notebook 2: Simple RAG\n",
    "\n",
    "In this notebook you will explore how to preform simple RAG (or nieve RAG) using semantic search and a chat model.\n",
    "\n",
    "## Learning Objectives\n",
    "- Learn how to perform a semantic search using Azure AI Search\n",
    "- Explore using hybrid search using Azure AI Search\n",
    "- Learn how to combine search results to perform RAG with an LLM\n",
    "\n",
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U agent-framework --pre -q\n",
    "%pip install -openai -q\n",
    "%pip install python-dotenv -q\n",
    "%pip install azure-search-documents -q\n",
    "%pip install azure-identity -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1af6ec",
   "metadata": {},
   "source": [
    "### Setup the Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12ddfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from agent_framework.azure import AzureOpenAIChatClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600673b7",
   "metadata": {},
   "source": [
    "### Get the needed environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c16b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4391d08",
   "metadata": {},
   "source": [
    "Define a method to get the embeddings for the queries we are going to be asking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2593e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSIONS = 1536 #3072 for 3-large\n",
    "\n",
    "def get_embeddings(text: str) -> list[float]:\n",
    "    if not text or text.strip() == \"\":\n",
    "        # Return zero vector for empty text\n",
    "        return [0.0] * EMBEDDING_DIMENSIONS\n",
    "    \n",
    "    # Truncate text if too long (max ~8000 tokens for ada-002)\n",
    "    max_chars = 30000  # Approximate character limit\n",
    "    if len(text) > max_chars:\n",
    "        text = text[:max_chars]\n",
    "    \n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_key=api_key,\n",
    "        api_version=api_version\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=embedding_deployment\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        # Return zero vector on error\n",
    "        return [0.0] * EMBEDDING_DIMENSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec4615",
   "metadata": {},
   "source": [
    "Define a method for performing the searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c17e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(search_key),\n",
    ")\n",
    "\n",
    "def do_search(query: str, use_hybrid: bool = True, top_k: int = 3) -> str:\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Hybrid search: {use_hybrid}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    q_vector = get_embeddings(query)\n",
    "\n",
    "    vq = VectorizedQuery(\n",
    "        vector=q_vector,\n",
    "        fields=\"BodyEmbeddings,AnswerEmbeddings\",\n",
    "    )\n",
    "\n",
    "    if use_hybrid:\n",
    "        results = search_client.search(\n",
    "            search_text=query, \n",
    "            vector_queries=[vq],\n",
    "            top=top_k,\n",
    "        )\n",
    "    else:\n",
    "        results = search_client.search(\n",
    "            search_text=None,\n",
    "            vector_queries=[vq],\n",
    "            top=top_k,\n",
    "        )\n",
    "\n",
    "    # Format results for LLM consumption\n",
    "    formatted_results = []\n",
    "    \n",
    "    for i, doc in enumerate(results):\n",
    "        score = doc.get(\"@search.score\", None)\n",
    "        \n",
    "        # Print to console (original behavior)\n",
    "        # print(f\"[{i}] id={doc['Id']}  score={score:.4f}\" if score is not None else f\"[{i}] id={doc['Id']}\")\n",
    "        # print(doc[\"Subject\"])\n",
    "        # print(doc[\"Body\"])\n",
    "        # print(doc[\"Answer\"])\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Build JSON object for each result\n",
    "        result_obj = {\n",
    "            \"document_number\": i + 1,\n",
    "            \"id\": doc.get(\"Id\"),\n",
    "            \"body\": doc.get(\"Body\"),\n",
    "            \"answer\": doc.get(\"Answer\"),\n",
    "            \"type\": doc.get(\"Type\"),\n",
    "            \"department\": doc.get(\"Queue\"),\n",
    "            \"priority\": doc.get(\"Priority\"),\n",
    "            \"business_type\": doc.get(\"Business_Type\"),\n",
    "            \"search_score\": score\n",
    "        }\n",
    "        \n",
    "        formatted_results.append(result_obj)\n",
    "    \n",
    "    # Return as formatted JSON string for RAG\n",
    "    rag_context = json.dumps(formatted_results, indent=2, ensure_ascii=False)\n",
    "\n",
    "    #     # Build formatted string for each result\n",
    "    #     result_str = f\"Document {i+1}:\\n{doc['Subject']}\\n{doc['Body']}\\nAnswer: {doc['Answer']}\"\n",
    "    #     formatted_results.append(result_str)\n",
    "    \n",
    "    # # Combine all results into a single string for RAG\n",
    "    # rag_context = \"\\n\\n\".join(formatted_results)\n",
    "    print(\"RAG Context:\")\n",
    "    print(rag_context)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    return rag_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc16f76",
   "metadata": {},
   "source": [
    "Define a prompts for system instructions and to use with the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa4fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI assistant that helps users learn information from the IT support tickent knowledge base.\n",
    "Answer the question using only the provided context.\n",
    "Use bullets if the answer has multiple points.\n",
    "If the answer is longer than 3 sentences, provide a summary.\n",
    "Answer ONLY with the facts listed in the list of sources provided in the context with the user query. \n",
    "Cite your source when you answer the question with the format [source-id].\n",
    "If the answer is not contained within the context, respond with \"I don't know.\"\n",
    "\"\"\"\n",
    "\n",
    "RAG_PROMPT = \"\"\"\n",
    "User Question: {user_query}\n",
    "Context:\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb881b2b",
   "metadata": {},
   "source": [
    "Queries to try:\n",
    "- \"What problems are there with Surface devices?\",\n",
    "- \"What sort of AWS problems have been reported?\",\n",
    "- \"Are there any issues logged for Dell XPS laptops?\"\n",
    "- \"Do we have more issues with MacBook Air computers or Dell XPS laptops?\",\n",
    "- \"What issues do we have with dell xps laptops?\",\n",
    "- \"What issues are for Dell XPS laptops and the user tried Win + Ctrl + Shift + B?\",\n",
    "- \"How many tickets were logged and Incidents for Human Resources and low priority?\",\n",
    "- \"Which Dell XPS issue does not mention Windows?\",\n",
    "- \"What department had consultants with Login Issues?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94919e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer is Human Resources\n",
    "#user_query = \"What department had consultants with Login Issues?\"\n",
    "\n",
    "# answer is 3 - won't get this one right\n",
    "user_query = \"How many tickets were logged and Incidents for Human Resources and low priority?\" \n",
    "\n",
    "# Hybrid on:\n",
    "context = do_search(user_query, use_hybrid=True, top_k=3)\n",
    "\n",
    "# Hybrid off:\n",
    "#context = do_search(user_query, use_hybrid=False, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AzureOpenAIChatClient(credential=DefaultAzureCredential()).create_agent(\n",
    "    instructions=SYSTEM_PROMPT,\n",
    "    name=\"rag-agent\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae359f",
   "metadata": {},
   "source": [
    "Now call the LLM with the original user question and the search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_context = RAG_PROMPT.format(\n",
    "    user_query=user_query,\n",
    "    context=context)\n",
    "\n",
    "result = await agent.run(rag_context)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18931fa6",
   "metadata": {},
   "source": [
    "Play around with the code above and try changing the following:\n",
    "- top_k value - default is 3, but try more and less to see if there is a difference\n",
    "- Hybrid - see if a hybrid search makes any difference\n",
    "- user_query - try some of the other questions listed (or you own)\n",
    "\n",
    "For Example - this query won't get a correct answer:\n",
    "'''\n",
    "user_query = \"How many tickets were logged and Incidents for Human Resources and low priority?\" \n",
    "'''\n",
    "\n",
    "See if you can figure out why or how to fix it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
