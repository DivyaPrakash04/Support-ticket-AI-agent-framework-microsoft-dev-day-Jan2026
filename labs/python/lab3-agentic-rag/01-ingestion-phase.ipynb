{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60524ce",
   "metadata": {},
   "source": [
    "# Notebook 1: The Ingestion Phase\n",
    "\n",
    "As we discussed in the presentation, the ingestion phase is basically the loading of the data sources the retrieval system uses. These data sources can be existing databases with structured data, however in this notebook we'll focus on unstructured data (such as documents).\n",
    "\n",
    "## Learning Objectives\n",
    "- Learn how to chunk markdown files into smaller sizes\n",
    "- Learn how the text chunking size provides different quality retrieval results in a RAG application\n",
    "- Learn how different embeddings models provide different results\n",
    "- Learn how to load an Azure AI Search index for a Vector Store\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "> NOTE: We need to use Semantic Kernel in this notebook in order to work with the embeddings and chunking (those features are not yet in Agent Framework as of the beginning of Jan 2026)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84bc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U agent-framework --pre -q\n",
    "%pip install -U semantic-kernel -q\n",
    "%pip install python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2e24b",
   "metadata": {},
   "source": [
    "## Step 1: Chunk files into smaller pieces\n",
    "\n",
    "### Document Chunking \n",
    "\n",
    "The process of taking a document and splitting into pieces is often referred to as \"chunking\". There are many ways to split a document and it isn't a *one-size-fits-all* activity, so you need to keep in mind how a document needs to be split in order to provide the most valuable chunks for your retrieval system.\n",
    "\n",
    "Important things to remember about these chunks:\n",
    "\n",
    "- We will get embeddings for each chunk\n",
    "- Relevant chunks will be found by a similarity search using embeddings\n",
    "- Often times an overlap of 10 - 20% is used if there is not a clean way to split the document\n",
    "- When working with real documents, you may need to address tables and images (images typically have different embedding models or need to be *verbalized*)\n",
    "- Each chunk needs to fit in the context window of the LLM, and keep in mind things can get lost in the middle when the context is too big\n",
    "- You may need to modify your chunking to improve the retrieval quality of your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b27ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.text import text_chunker\n",
    "from typing import List\n",
    "\n",
    "async def chunk_markdown_file(file_path: str, max_token_per_line: int = 256):\n",
    "    \"\"\"\n",
    "    Reads a markdown file and chunks it into smaller pieces using Semantic Kernel's text_chunker.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the markdown file\n",
    "        max_token_per_line: Maximum number of tokens per line\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Read the markdown file from the file system\n",
    "    print(f\"Reading file: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            markdown_content = file.read()\n",
    "        print(f\"Successfully read file. Total characters: {len(markdown_content)}\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Step 2: Use Semantic Kernel's text_chunker to split into smaller pieces\n",
    "    print(f\"Chunking text with max_token_per_line={max_token_per_line}...\\n\")\n",
    "        \n",
    "    # Split the text into chunks\n",
    "    chunks = text_chunker.split_markdown_lines(\n",
    "        text=markdown_content, \n",
    "        max_token_per_line=max_token_per_line,\n",
    "    )\n",
    "    \n",
    "    # Step 3: Capture all chunks into a list variable\n",
    "    chunk_list: List[str] = list(chunks)\n",
    "    \n",
    "    print(f\"Total chunks created: {len(chunk_list)}\\n\")\n",
    "    \n",
    "    # Step 4: Print out the first 3 chunks (or fewer if less than 3 exist)\n",
    "    chunks_to_display = min(3, len(chunk_list))\n",
    "    print(f\"Displaying first {chunks_to_display} chunks:\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i in range(chunks_to_display):\n",
    "        print(f\"\\n--- Chunk {i + 1} ---\")\n",
    "        print(f\"Length: {len(chunk_list[i])} characters\")\n",
    "        print(f\"Content:\\n{chunk_list[i]}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "   \n",
    "    \n",
    "    return chunk_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a2cb7",
   "metadata": {},
   "source": [
    "Next, you can now use the above method to split the sample markdown file (in the **/data** folder) into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b104035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your markdown file\n",
    "markdown_file_path = \"data/sample.md\"\n",
    "\n",
    "# Chunk the markdown file\n",
    "chunks = await chunk_markdown_file(\n",
    "    file_path=markdown_file_path,\n",
    "    max_token_per_line=256,  # Adjust chunk size as needed\n",
    ")\n",
    "\n",
    "if chunks:\n",
    "     # Print summary statistics\n",
    "    if chunks:\n",
    "        avg_chunk_size = sum(len(chunk) for chunk in chunks) / len(chunks)\n",
    "        print(f\"\\nChunking Summary:\")\n",
    "        print(f\"  - Total chunks: {len(chunks)}\")\n",
    "        print(f\"  - Average chunk size: {avg_chunk_size:.2f} characters\")\n",
    "        print(f\"  - Smallest chunk: {len(min(chunks, key=len))} characters\")\n",
    "        print(f\"  - Largest chunk: {len(max(chunks, key=len))} characters\")\n",
    "\n",
    "# The chunks list is now available for use in the next notebook\n",
    "print(f\"\\n‚úÖ Chunks are now stored in the 'chunks' variable for use in the next step.\")\n",
    "print(f\"   You can access individual chunks with chunks[0], chunks[1], etc.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3261fb",
   "metadata": {},
   "source": [
    "### Try Using LangChain's MarkdownHeaderTextSplitter (optional)\n",
    "\n",
    "LangChain is another popular python package used with RAG applications. They have many options than Semantic Kernal. In the code below you'll explore the MarkdownHeaderTextSplitter may provide a better splitter option for you.\n",
    "\n",
    "First you'll need to install the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db6dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain -q\n",
    "%pip install langchain-core -q\n",
    "%pip install langchain-text-splitters -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276024f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from typing import List, Dict\n",
    "\n",
    "def chunk_markdown_file(file_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Read a markdown file and split it into chunks based on headers.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the markdown file\n",
    "    \n",
    "    Returns:\n",
    "        List of document chunks with metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Read the markdown file\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            markdown_content = file.read()\n",
    "        print(f\"Successfully read file: {file_path}\")\n",
    "        print(f\"File size: {len(markdown_content)} characters\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Step 2: Configure the MarkdownHeaderTextSplitter\n",
    "    # Define which headers to split on and their metadata keys\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),      # H1 headers\n",
    "        (\"##\", \"Header 2\"),     # H2 headers\n",
    "        (\"###\", \"Header 3\"),    # H3 headers\n",
    "    ]\n",
    "    \n",
    "    # Create the splitter instance\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False  # Keep headers in the content\n",
    "    )\n",
    "    \n",
    "    # Step 3: Split the document and capture chunks\n",
    "    chunks = markdown_splitter.split_text(markdown_content)\n",
    "    \n",
    "    # Convert to list of dictionaries for easier handling\n",
    "    chunk_list = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_dict = {\n",
    "            'index': i,\n",
    "            'content': chunk.page_content,\n",
    "            'metadata': chunk.metadata,\n",
    "            'length': len(chunk.page_content)\n",
    "        }\n",
    "        chunk_list.append(chunk_dict)\n",
    "    \n",
    "    print(f\"Total number of chunks created: {len(chunk_list)}\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 4: Print the first 3 chunks (or fewer if less than 3 exist)\n",
    "    chunks_to_display = min(3, len(chunk_list))\n",
    "    \n",
    "    for i in range(chunks_to_display):\n",
    "        chunk = chunk_list[i]\n",
    "        print(f\"\\nüìÑ CHUNK {i + 1}:\")\n",
    "        print(f\"   Metadata: {chunk['metadata']}\")\n",
    "        print(f\"   Length: {chunk['length']} characters\")\n",
    "        print(f\"   Content preview:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Display first 300 characters of content (or full if shorter)\n",
    "        content_preview = chunk['content'][:300]\n",
    "        if len(chunk['content']) > 300:\n",
    "            content_preview += \"...\"\n",
    "        print(content_preview)\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Return the full list of chunks for use in next lab\n",
    "    return chunk_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23944e57",
   "metadata": {},
   "source": [
    "Next, you can now use the above method to split the same sample markdown file (in the **/data** folder) into chunks.\n",
    "\n",
    "> NOTE: the LangChain splitter splits on sections and provided metadata about the section hierarchy (which may be useful for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cafea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your markdown file\n",
    "markdown_file_path = \"data/sample.md\"\n",
    "\n",
    "# Chunk the markdown file\n",
    "lc_chunks = chunk_markdown_file(markdown_file_path)\n",
    "if lc_chunks:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä SUMMARY STATISTICS:\")\n",
    "    print(f\"   Total chunks: {len(chunks)}\")\n",
    "    \n",
    "    total_chars = sum(chunk['length'] for chunk in lc_chunks)\n",
    "    avg_chunk_size = total_chars / len(chunks) if chunks else 0\n",
    "    print(f\"   Average chunk size: {avg_chunk_size:.1f} characters\")\n",
    "    \n",
    "    max_chunk = max(lc_chunks, key=lambda x: x['length'])\n",
    "    min_chunk = min(lc_chunks, key=lambda x: x['length'])\n",
    "    print(f\"   Largest chunk: {max_chunk['length']} characters (chunk #{max_chunk['index']})\")\n",
    "    print(f\"   Smallest chunk: {min_chunk['length']} characters (chunk #{min_chunk['index']})\")\n",
    "\n",
    "# The chunks list is now available for use in the next notebook\n",
    "print(f\"\\n‚úÖ Chunks are now stored in the 'chunks' variable for use in the next step.\")\n",
    "print(f\"   You can access individual chunks with chunks[0], chunks[1], etc.\")\n",
    "\n",
    "# If going to use the LangChain chunks in the next step, set the chunks variable to the chunk text\n",
    "chunks = [item[\"content\"] for item in lc_chunks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d806d",
   "metadata": {},
   "source": [
    "## Step 2: Create Embeddings for Semantic Searches\n",
    "\n",
    "In this step you'll use AzureOpenAI to create the embeddings for the chunks you created above - you'll need to decide with chunking technique you like best.\n",
    "\n",
    "The code below will utilize the older text-embedding-ada-002 model for creating the embeddings. In Step 4, you'll get to compare the embeddings from OpenAI how they differ in a semantic search.\n",
    "\n",
    "First you'll need to install the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561eacdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5505b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),  \n",
    "    api_version=\"2024-02-01\",  \n",
    ")\n",
    "\n",
    "def embed_chunks(text_chunks: list[str], model: str) -> list[list[float]]:\n",
    "    # model is your Azure deployment name, e.g. \"embeddings-prod\"\n",
    "    response = client.embeddings.create(\n",
    "        model=model,          # deployment name, not the base model id[web:42]\n",
    "        input=text_chunks,    # list of chunk strings\n",
    "    )\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eaf80c",
   "metadata": {},
   "source": [
    "Next you use the above utility to create embeddings of the chunks (created earlier) and take a look at a few of the returned vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c176c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = embed_chunks(chunks, model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Single embedding (first item)\n",
    "print(\"First embedding:\")\n",
    "print(embeddings[0])  # a list[float]\n",
    "print(len(embeddings[0]), \"dimensions\")\n",
    "\n",
    "# First two embeddings\n",
    "print(\"\\nFirst two embeddings:\")\n",
    "for i, emb in enumerate(embeddings[:2]):  # slice to first 2[web:68][web:72]\n",
    "    print(f\"Embedding {i}:\")\n",
    "    print(emb[:8], \"...\")  # show just first few dims to keep output short\n",
    "    print(\"dim:\", len(emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e6c13",
   "metadata": {},
   "source": [
    "## Step 3: Load Azure AI Search Index\n",
    "\n",
    "Next step is the inserting of the chunks and embeddings into a vector database. In this step we'll use Azure AI Search as the vector database.\n",
    "\n",
    "First you'll need to install the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-search-documents -q\n",
    "%pip install azure-identity -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f660dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    "    HnswAlgorithmConfiguration,\n",
    ")\n",
    "\n",
    "search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "my_initials = os.getenv(\"MY_INITIALS\")\n",
    "index_name = f\"{my_initials.lower()}vectorindex\"\n",
    "embedding_dimension = 1536  # e.g. 1536 for ada-002\n",
    "\n",
    "# Create SearchClient for later use\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(search_key),\n",
    ")\n",
    "\n",
    "def ensure_chunk_vector_index() -> None:\n",
    "    \"\"\"\n",
    "    Ensure an Azure AI Search index exists for chunk text + embeddings.\n",
    "    If it does not exist, create it. If it exists, do nothing.\n",
    "    \"\"\"\n",
    "    if not my_initials:\n",
    "        raise ValueError(\"MY_INITIALS environment variable must be set in order to prevent index name collisions.\")\n",
    "\n",
    "    if not search_endpoint or not search_key:\n",
    "        raise ValueError(\"AZURE_SEARCH_ENDPOINT and AZURE_SEARCH_API_KEY must be set or passed in.\")\n",
    "\n",
    "    credential = AzureKeyCredential(search_key)\n",
    "    index_client = SearchIndexClient(endpoint=search_endpoint, credential=credential)\n",
    "\n",
    "    # Check if the index already exists\n",
    "    existing_names = list(index_client.list_index_names())\n",
    "    if index_name in existing_names:\n",
    "        print(f\"Index '{index_name}' already exists; skipping creation.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Index '{index_name}' does not exist; creating now...\")\n",
    "\n",
    "    fields = [\n",
    "        SimpleField(\n",
    "            name=\"id\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            key=True,\n",
    "            filterable=True,\n",
    "            sortable=True,\n",
    "            facetable=True,\n",
    "        ),\n",
    "        SearchableField(\n",
    "            name=\"content\",\n",
    "            type=SearchFieldDataType.String,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"contentVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=embedding_dimension,\n",
    "            vector_search_profile_name=\"chunk-vector-profile\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"chunk-hnsw-config\",\n",
    "                kind=\"hnsw\",\n",
    "            )\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"chunk-vector-profile\",\n",
    "                algorithm_configuration_name=\"chunk-hnsw-config\",\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "    )\n",
    "\n",
    "    result = index_client.create_index(index)  # only create because we know it doesn't exist\n",
    "    print(f\"Index '{result.name}' created.\")\n",
    "\n",
    "\n",
    "def upload_chunks_with_embeddings(\n",
    "    chunks: List[str],\n",
    "    embeddings: List[List[float]],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Upload chunks and their corresponding embeddings to the Azure AI Search index.\n",
    "    \"\"\"\n",
    "    if len(chunks) != len(embeddings):\n",
    "        raise ValueError(\"chunks and embeddings must have the same length\")\n",
    "\n",
    "    docs = []\n",
    "    for i, (text, vector) in enumerate(zip(chunks, embeddings)):\n",
    "        docs.append(\n",
    "            {\n",
    "                \"@search.action\": \"mergeOrUpload\",  # or \"upload\" if you only insert[web:85][web:88]\n",
    "                \"id\": str(i),\n",
    "                \"content\": text,\n",
    "                \"contentVector\": vector,  # must match index vector field name & dimensions[web:87][web:91]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Azure AI Search supports up to 1,000 docs per batch; keep it small for now\n",
    "    batch_size = 1000\n",
    "    for start in range(0, len(docs), batch_size):\n",
    "        batch = docs[start : start + batch_size]\n",
    "        result = search_client.upload_documents(documents=batch)\n",
    "        # Optional: check status per doc\n",
    "        succeeded = sum(1 for r in result if r.succeeded)\n",
    "        print(f\"Uploaded {succeeded}/{len(batch)} documents in batch starting at {start}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b02178",
   "metadata": {},
   "source": [
    "Next you can run the code that will ensure the index has been created and then load the chunks and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf9061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make sure the index exists\n",
    "ensure_chunk_vector_index()\n",
    "\n",
    "# chunks: list[str]  (your chunk strings from above - either Semantic Kernel or LangChain)\n",
    "# embeddings: list[list[float]] generated from Azure OpenAI for each chunk using text-embedding-ada-002\n",
    "upload_chunks_with_embeddings(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d061804f",
   "metadata": {},
   "source": [
    "Next let's see what the semantic search results would be for these questions:\n",
    "- ‚ÄúExplain the difference between supervised, unsupervised, and reinforcement learning.‚Äù\n",
    "- ‚ÄúWhat kinds of real‚Äëworld problems can machine learning solve today?‚Äù\n",
    "- ‚ÄúHow does reinforcement learning decide which actions to take to maximize rewards?‚Äù\n",
    "- ‚ÄúGive some examples of how machine learning is used in healthcare and fraud prevention.‚Äù\n",
    "- ‚ÄúWhy is machine learning becoming more important as the amount of data grows?‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "aoai_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=\"2024-02-01\",\n",
    ")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(search_key),\n",
    ")\n",
    "\n",
    "def embed_query(text: str, model: str = \"text-embedding-ada-002\") -> List[float]:\n",
    "    \"\"\"Create a single embedding vector for a query string using Azure OpenAI.\"\"\"\n",
    "    resp = aoai_client.embeddings.create(\n",
    "        model=model,        # Azure deployment name\n",
    "        input=[text],\n",
    "    )\n",
    "    return resp.data[0].embedding\n",
    "\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "def run_test_queries(queries: list[str], use_hybrid: bool = True, top_k: int = 3):\n",
    "    for q in queries:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Query: {q}\")\n",
    "        print(f\"Hybrid search: {use_hybrid}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        q_vector = embed_query(q)\n",
    "\n",
    "        vq = VectorizedQuery(\n",
    "            vector=q_vector,\n",
    "            fields=\"contentVector\",\n",
    "        )\n",
    "\n",
    "        if use_hybrid:\n",
    "            results = search_client.search(\n",
    "                search_text=q,\n",
    "                vector_queries=[vq],\n",
    "                top=top_k,\n",
    "            )\n",
    "        else:\n",
    "            results = search_client.search(\n",
    "                search_text=None,\n",
    "                vector_queries=[vq],\n",
    "                top=top_k,\n",
    "            )\n",
    "\n",
    "        for i, doc in enumerate(results):\n",
    "            score = doc.get(\"@search.score\", None)  # float relevance score\n",
    "            print(f\"[{i}] id={doc['id']}  score={score:.4f}\" if score is not None else f\"[{i}] id={doc['id']}\")\n",
    "            print(doc[\"content\"])\n",
    "            print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf263dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_QUERIES = [\n",
    "    \"Explain the difference between supervised, unsupervised, and reinforcement learning.\",\n",
    "    #\"What kinds of real-world problems can machine learning solve today?\",\n",
    "    #\"How does reinforcement learning decide which actions to take to maximize rewards?\",\n",
    "    #\"Give some examples of how machine learning is used in healthcare and fraud prevention.\",\n",
    "    #\"Why is machine learning becoming more important as the amount of data grows?\",\n",
    "]\n",
    "\n",
    "# Hybrid on:\n",
    "run_test_queries(TEST_QUERIES, use_hybrid=True, top_k=3)\n",
    "\n",
    "# Hybrid off:\n",
    "run_test_queries(TEST_QUERIES, use_hybrid=False, top_k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ee907",
   "metadata": {},
   "source": [
    "## Step 4: Test Different Chunk Sizes and Embedding Models\n",
    "\n",
    "In this step, you get to explore the differences between using:\n",
    "- text-embedding-ada-002\n",
    "- text-embedding-3-small\n",
    "- text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186edc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "\n",
    "class AzureSearchComparison:\n",
    "    \"\"\"\n",
    "    Compare semantic search results across different embedding model indexes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize Azure Search connection.\n",
    "        \"\"\"\n",
    "        self.endpoint = search_endpoint\n",
    "        self.credential = AzureKeyCredential(search_key)\n",
    "        \n",
    "        # Define the indexes and their embedding dimensions\n",
    "        self.indexes = {\n",
    "            'large3index': {\n",
    "                'dimensions': 3072,  # Corrected dimension for text-embedding-3-large\n",
    "                'model': 'text-embedding-3-large'\n",
    "            },\n",
    "            'small3index': {\n",
    "                'dimensions': 1536,  # Corrected dimension for text-embedding-3-small\n",
    "                'model': 'text-embedding-3-small'\n",
    "            },\n",
    "            'ada002index': {\n",
    "                'dimensions': 1536,\n",
    "                'model': 'text-embedding-ada-002'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_embedding(self, text: str, model: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Generate embedding for the query text using specified model.\n",
    "        \n",
    "        Args:\n",
    "            text: Query text to embed\n",
    "            model: OpenAI embedding model name\n",
    "            \n",
    "        Returns:\n",
    "            List of floats representing the embedding vector\n",
    "        \"\"\"\n",
    "        from openai import OpenAI\n",
    "        \n",
    "        client = AzureOpenAI(\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_version=\"2024-02-01\",\n",
    "        )\n",
    "                \n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    def search_index(self, \n",
    "                     index_name: str, \n",
    "                     query_text: str, \n",
    "                     vector_dimensions: int,\n",
    "                     embedding_model: str,\n",
    "                     top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search a specific Azure AI Search index using vector similarity.\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the Azure search index\n",
    "            query_text: Text query to search for\n",
    "            vector_dimensions: Dimension of the embedding vectors\n",
    "            embedding_model: OpenAI model to use for generating query embedding\n",
    "            top_k: Number of top results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of search results with scores\n",
    "        \"\"\"\n",
    "        # Create search client for this index\n",
    "        search_client = SearchClient(\n",
    "            endpoint=self.endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=self.credential\n",
    "        )\n",
    "        \n",
    "        # Generate embedding for the query\n",
    "        print(f\"  Generating embedding with {embedding_model}...\")\n",
    "        query_vector = self.get_embedding(query_text, embedding_model)\n",
    "        \n",
    "        # Create vector query\n",
    "        vector_query = VectorizedQuery(\n",
    "            vector=query_vector,\n",
    "            k=top_k,\n",
    "            fields=\"contentVector\"\n",
    "        )\n",
    "        \n",
    "        # Perform search\n",
    "        results = search_client.search(\n",
    "            search_text=None,  # Pure vector search\n",
    "            vector_queries=[vector_query],\n",
    "            select=[\"id\", \"content\"],\n",
    "            top=top_k\n",
    "        )\n",
    "        \n",
    "        # Collect results\n",
    "        search_results = []\n",
    "        for result in results:\n",
    "            search_results.append({\n",
    "                'id': result['id'],\n",
    "                'content': result['content'][:200] + \"...\" if len(result['content']) > 200 else result['content'],\n",
    "                'score': result['@search.score']\n",
    "            })\n",
    "        \n",
    "        return search_results\n",
    "    \n",
    "    def compare_search_results(self, query_text: str, top_k: int = 3):\n",
    "        \"\"\"\n",
    "        Compare search results across all three indexes.\n",
    "        \n",
    "        Args:\n",
    "            query_text: The search query\n",
    "            top_k: Number of top results to show per index\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üîç SEMANTIC SEARCH COMPARISON\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nQuery: '{query_text}'\\n\")\n",
    "        print(f\"Retrieving top {top_k} results from each index...\\n\")\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        # Search each index\n",
    "        for index_name, index_info in self.indexes.items():\n",
    "            print(f\"\\nüìä Searching {index_name.upper()}\")\n",
    "            print(f\"   Model: {index_info['model']}\")\n",
    "            print(f\"   Dimensions: {index_info['dimensions']}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            try:\n",
    "                results = self.search_index(\n",
    "                    index_name=index_name,\n",
    "                    query_text=query_text,\n",
    "                    vector_dimensions=index_info['dimensions'],\n",
    "                    embedding_model=index_info['model'],\n",
    "                    top_k=top_k\n",
    "                )\n",
    "                \n",
    "                all_results[index_name] = results\n",
    "                \n",
    "                # Display results for this index\n",
    "                for i, result in enumerate(results, 1):\n",
    "                    print(f\"\\n   Result {i} (Score: {result['score']:.4f}):\")\n",
    "                    print(f\"   ID: {result['id']}\")\n",
    "                    print(f\"   Content: {result['content']}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error searching {index_name}: {str(e)}\")\n",
    "                all_results[index_name] = []\n",
    "        \n",
    "        # Compare and analyze differences\n",
    "        self.analyze_differences(all_results, query_text)\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def analyze_differences(self, all_results: Dict[str, List[Dict]], query_text: str):\n",
    "        \"\"\"\n",
    "        Analyze and highlight differences between search results.\n",
    "        \n",
    "        Args:\n",
    "            all_results: Dictionary of results from each index\n",
    "            query_text: Original query text\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìà ANALYSIS: Differences Between Embedding Models\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Check if all indexes returned results\n",
    "        indexes_with_results = [idx for idx, results in all_results.items() if results]\n",
    "        \n",
    "        if len(indexes_with_results) < 2:\n",
    "            print(\"\\n‚ö†Ô∏è  Not enough results to compare. Check your indexes and API keys.\")\n",
    "            return\n",
    "        \n",
    "        # Compare top results\n",
    "        print(\"\\nüéØ Top Result Comparison:\")\n",
    "        print(\"-\" * 40)\n",
    "        for index_name, results in all_results.items():\n",
    "            if results:\n",
    "                top_result = results[0]\n",
    "                print(f\"\\n{index_name}:\")\n",
    "                print(f\"  Top match ID: {top_result['id']}\")\n",
    "                print(f\"  Score: {top_result['score']:.4f}\")\n",
    "        \n",
    "        # Check for agreement on top result\n",
    "        top_ids = [results[0]['id'] for results in all_results.values() if results]\n",
    "        if len(set(top_ids)) == 1:\n",
    "            print(\"\\n‚úÖ All models agree on the top result!\")\n",
    "        else:\n",
    "            print(\"\\nüîÑ Models returned different top results\")\n",
    "            \n",
    "        # Calculate overlap in results\n",
    "        print(\"\\nüìä Result Overlap Analysis:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Get all unique IDs per index\n",
    "        for i, (idx1, results1) in enumerate(all_results.items()):\n",
    "            if not results1:\n",
    "                continue\n",
    "            ids1 = set(r['id'] for r in results1)\n",
    "            \n",
    "            for idx2, results2 in list(all_results.items())[i+1:]:\n",
    "                if not results2:\n",
    "                    continue\n",
    "                ids2 = set(r['id'] for r in results2)\n",
    "                \n",
    "                overlap = ids1.intersection(ids2)\n",
    "                overlap_pct = (len(overlap) / max(len(ids1), len(ids2))) * 100\n",
    "                \n",
    "                print(f\"\\n{idx1} vs {idx2}:\")\n",
    "                print(f\"  Overlapping results: {len(overlap)}/{max(len(ids1), len(ids2))}\")\n",
    "                print(f\"  Similarity: {overlap_pct:.1f}%\")\n",
    "                \n",
    "                if overlap:\n",
    "                    print(f\"  Common IDs: {', '.join(sorted(overlap))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d787d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the comparison tool\n",
    "searcher = AzureSearchComparison()\n",
    "\n",
    "# The search query\n",
    "query = \"Explain the difference between supervised, unsupervised, and reinforcement learning.\"\n",
    "\n",
    "# Run the comparison\n",
    "print(\"\\nüöÄ Starting semantic search comparison across embedding models...\")\n",
    "results = searcher.compare_search_results(query, top_k=3)\n",
    "\n",
    "# Additional insights\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° KEY INSIGHTS FOR STUDENTS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "1. DIMENSIONALITY: \n",
    "    - text-embedding-3-large (3072 dims) captures more nuanced relationships\n",
    "    - text-embedding-ada-002 and text-embedding-3-small (1536 dims) are more efficient\n",
    "\n",
    "2. PERFORMANCE VS COST:\n",
    "    - Larger models may provide better semantic understanding\n",
    "    - Smaller models are faster and cheaper to run at scale\n",
    "\n",
    "3. USE CASE CONSIDERATIONS:\n",
    "    - For high-precision tasks: Consider larger embedding models\n",
    "    - For high-throughput applications: Smaller models may be sufficient\n",
    "    - Always test with your specific data and queries\n",
    "\n",
    "4. WHAT TO LOOK FOR:\n",
    "    - Do all models find the same top result?\n",
    "    - How much overlap is there in the top 3 results?\n",
    "    - Are the relevance scores significantly different?\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
