{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60524ce",
   "metadata": {},
   "source": [
    "# Notebook 1: The Ingestion Phase\n",
    "\n",
    "As we discussed in the presentation, the ingestion phase is basically the loading of the data sources the retrieval system uses. These data sources can be existing databases with structured data, however in this notebook we'll focus on unstructured data (such as documents).\n",
    "\n",
    "## Learning Objectives\n",
    "- Learn how to chunk markdown files into smaller sizes\n",
    "- Learn how the text chunking size provides different quality retrieval results in a RAG application\n",
    "- Learn how different embeddings models provide different results\n",
    "- Learn how to load an Azure AI Search index for a Vector Store\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "> NOTE: We need to use Semantic Kernel in this notebook in order to work with the embeddings and chunking (those features are not yet in Agent Framework as of the beginning of Jan 2026)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84bc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U semantic-kernel -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2e24b",
   "metadata": {},
   "source": [
    "## Step 1: Chunk files into smaller pieces\n",
    "\n",
    "### Document Chunking \n",
    "\n",
    "The process of taking a document and splitting into pieces is often referred to as \"chunking\". There are many ways to split a document and it isn't a *one-size-fits-all* activity, so you need to keep in mind how a document needs to be split in order to provide the most valuable chunks for your retrieval system.\n",
    "\n",
    "Important things to remember about these chunks:\n",
    "\n",
    "- We will get embeddings for each chunk\n",
    "- Relevant chunks will be found by a similarity search using embeddings\n",
    "- Often times an overlap of 10 - 20% is used if there is not a clean way to split the document\n",
    "- When working with real documents, you may need to address tables and images (images typically have different embedding models or need to be *verbalized*)\n",
    "- Each chunk needs to fit in the context window of the LLM, and keep in mind things can get lost in the middle when the context is too big\n",
    "- You may need to modify your chunking to improve the retrieval quality of your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49b27ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.text import text_chunker\n",
    "from typing import List\n",
    "\n",
    "async def chunk_markdown_file(file_path: str, max_token_per_line: int = 256):\n",
    "    \"\"\"\n",
    "    Reads a markdown file and chunks it into smaller pieces using Semantic Kernel's text_chunker.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the markdown file\n",
    "        max_token_per_line: Maximum number of tokens per line\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Read the markdown file from the file system\n",
    "    print(f\"Reading file: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            markdown_content = file.read()\n",
    "        print(f\"Successfully read file. Total characters: {len(markdown_content)}\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Step 2: Use Semantic Kernel's text_chunker to split into smaller pieces\n",
    "    print(f\"Chunking text with max_token_per_line={max_token_per_line}...\\n\")\n",
    "        \n",
    "    # Split the text into chunks\n",
    "    chunks = text_chunker.split_markdown_lines(\n",
    "        text=markdown_content, \n",
    "        max_token_per_line=max_token_per_line,\n",
    "    )\n",
    "    \n",
    "    # Step 3: Capture all chunks into a list variable\n",
    "    chunk_list: List[str] = list(chunks)\n",
    "    \n",
    "    print(f\"Total chunks created: {len(chunk_list)}\\n\")\n",
    "    \n",
    "    # Step 4: Print out the first 3 chunks (or fewer if less than 3 exist)\n",
    "    chunks_to_display = min(3, len(chunk_list))\n",
    "    print(f\"Displaying first {chunks_to_display} chunks:\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i in range(chunks_to_display):\n",
    "        print(f\"\\n--- Chunk {i + 1} ---\")\n",
    "        print(f\"Length: {len(chunk_list[i])} characters\")\n",
    "        print(f\"Content:\\n{chunk_list[i]}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "   \n",
    "    \n",
    "    return chunk_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a2cb7",
   "metadata": {},
   "source": [
    "Next, you can now use the above method to split the sample markdown file (in the **labs/assets** folder) into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b104035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: ../../assets/sample.md\n",
      "Successfully read file. Total characters: 1410\n",
      "\n",
      "Chunking text with max_token_per_line=256...\n",
      "\n",
      "Total chunks created: 2\n",
      "\n",
      "Displaying first 2 chunks:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Length: 656 characters\n",
      "Content:\n",
      "# Introduction to Machine Learning\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing computer programs that can access data and use it to learn for themselves.\n",
      "\n",
      "## Types of Machine Learning\n",
      "\n",
      "### Supervised Learning\n",
      "\n",
      "In supervised learning, the algorithm learns from labeled training data. Each training example consists of an input object and a desired output value. The algorithm analyzes the training data and produces an inferred function.\n",
      "\n",
      "### Unsupervised Learning\n",
      "\n",
      "Unsupervised learning algorithms work with unlabeled data.\n",
      "----------------------------------------\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Length: 753 characters\n",
      "Content:\n",
      "The system tries to learn without a teacher, finding hidden patterns or intrinsic structures in input data.\n",
      "\n",
      "### Reinforcement Learning\n",
      "\n",
      "Reinforcement learning is about taking suitable action to maximize reward in a particular situation. It is employed by various software and machines to find the best possible behavior or path in a specific situation.\n",
      "\n",
      "## Applications\n",
      "\n",
      "Machine learning has numerous applications including:\n",
      "- Image recognition and computer vision\n",
      "- Natural language processing\n",
      "- Recommendation systems\n",
      "- Fraud detection\n",
      "- Medical diagnosis\n",
      "- Autonomous vehicles\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "As we continue to generate more data, machine learning will become increasingly important for making sense of this information and automating complex tasks.\n",
      "----------------------------------------\n",
      "================================================================================\n",
      "\n",
      "Chunking Summary:\n",
      "  - Total chunks: 2\n",
      "  - Average chunk size: 704.50 characters\n",
      "  - Smallest chunk: 656 characters\n",
      "  - Largest chunk: 753 characters\n",
      "\n",
      "âœ… Chunks are now stored in the 'chunks' variable for use in the next step.\n",
      "   You can access individual chunks with chunks[0], chunks[1], etc.\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your markdown file\n",
    "markdown_file_path = \"../../assets/sample.md\"\n",
    "\n",
    "# Chunk the markdown file\n",
    "chunks = await chunk_markdown_file(\n",
    "    file_path=markdown_file_path,\n",
    "    max_token_per_line=256,  # Adjust chunk size as needed\n",
    ")\n",
    "\n",
    "if chunks:\n",
    "     # Print summary statistics\n",
    "    if chunks:\n",
    "        avg_chunk_size = sum(len(chunk) for chunk in chunks) / len(chunks)\n",
    "        print(f\"\\nChunking Summary:\")\n",
    "        print(f\"  - Total chunks: {len(chunks)}\")\n",
    "        print(f\"  - Average chunk size: {avg_chunk_size:.2f} characters\")\n",
    "        print(f\"  - Smallest chunk: {len(min(chunks, key=len))} characters\")\n",
    "        print(f\"  - Largest chunk: {len(max(chunks, key=len))} characters\")\n",
    "\n",
    "# The chunks list is now available for use in the next notebook\n",
    "print(f\"\\nâœ… Chunks are now stored in the 'chunks' variable for use in the next step.\")\n",
    "print(f\"   You can access individual chunks with chunks[0], chunks[1], etc.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3261fb",
   "metadata": {},
   "source": [
    "### Try Using LangChain's MarkdownHeaderTextSplitter (optional)\n",
    "\n",
    "LangChain is another popular python package used with RAG applications. They have more text splitter options than Semantic Kernal has. In the code below you'll explore the [MarkdownHeaderTextSplitter](https://reference.langchain.com/v0.3/python/text_splitters/markdown/langchain_text_splitters.markdown.MarkdownHeaderTextSplitter.html).\n",
    "\n",
    "First you'll need to install the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37db6dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain -q\n",
    "%pip install langchain-core -q\n",
    "%pip install langchain-text-splitters -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "276024f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from typing import List, Dict\n",
    "\n",
    "def chunk_markdown_file(file_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Read a markdown file and split it into chunks based on headers.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the markdown file\n",
    "    \n",
    "    Returns:\n",
    "        List of document chunks with metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Read the markdown file\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            markdown_content = file.read()\n",
    "        print(f\"Successfully read file: {file_path}\")\n",
    "        print(f\"File size: {len(markdown_content)} characters\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Step 2: Configure the MarkdownHeaderTextSplitter\n",
    "    # Define which headers to split on and their metadata keys\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),      # H1 headers\n",
    "        (\"##\", \"Header 2\"),     # H2 headers\n",
    "        (\"###\", \"Header 3\"),    # H3 headers\n",
    "    ]\n",
    "    \n",
    "    # Create the splitter instance\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False  # Keep headers in the content\n",
    "    )\n",
    "    \n",
    "    # Step 3: Split the document and capture chunks\n",
    "    chunks = markdown_splitter.split_text(markdown_content)\n",
    "    \n",
    "    # Convert to list of dictionaries for easier handling\n",
    "    chunk_list = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_dict = {\n",
    "            'index': i,\n",
    "            'content': chunk.page_content,\n",
    "            'metadata': chunk.metadata,\n",
    "            'length': len(chunk.page_content)\n",
    "        }\n",
    "        chunk_list.append(chunk_dict)\n",
    "    \n",
    "    print(f\"Total number of chunks created: {len(chunk_list)}\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 4: Print the first 3 chunks (or fewer if less than 3 exist)\n",
    "    chunks_to_display = min(3, len(chunk_list))\n",
    "    \n",
    "    for i in range(chunks_to_display):\n",
    "        chunk = chunk_list[i]\n",
    "        print(f\"\\nðŸ“„ CHUNK {i + 1}:\")\n",
    "        print(f\"   Metadata: {chunk['metadata']}\")\n",
    "        print(f\"   Length: {chunk['length']} characters\")\n",
    "        print(f\"   Content preview:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Display first 300 characters of content (or full if shorter)\n",
    "        content_preview = chunk['content'][:300]\n",
    "        if len(chunk['content']) > 300:\n",
    "            content_preview += \"...\"\n",
    "        print(content_preview)\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Return the full list of chunks for use in next lab\n",
    "    return chunk_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23944e57",
   "metadata": {},
   "source": [
    "Next, you can now use the above method to split the same sample markdown file (in the **/data** folder) into chunks.\n",
    "\n",
    "> NOTE: the LangChain splitter splits on sections and provided metadata about the section hierarchy (which may be useful for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3cafea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file: ../../assets/sample.md\n",
      "File size: 1410 characters\n",
      "\n",
      "Total number of chunks created: 6\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ“„ CHUNK 1:\n",
      "   Metadata: {'Header 1': 'Introduction to Machine Learning'}\n",
      "   Length: 287 characters\n",
      "   Content preview:\n",
      "----------------------------------------\n",
      "# Introduction to Machine Learning  \n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing computer programs that can access data and use it to learn for themselves.\n",
      "----------------------------------------\n",
      "\n",
      "ðŸ“„ CHUNK 2:\n",
      "   Metadata: {'Header 1': 'Introduction to Machine Learning', 'Header 2': 'Types of Machine Learning', 'Header 3': 'Supervised Learning'}\n",
      "   Length: 283 characters\n",
      "   Content preview:\n",
      "----------------------------------------\n",
      "## Types of Machine Learning  \n",
      "### Supervised Learning  \n",
      "In supervised learning, the algorithm learns from labeled training data. Each training example consists of an input object and a desired output value. The algorithm analyzes the training data and produces an inferred function.\n",
      "----------------------------------------\n",
      "\n",
      "ðŸ“„ CHUNK 3:\n",
      "   Metadata: {'Header 1': 'Introduction to Machine Learning', 'Header 2': 'Types of Machine Learning', 'Header 3': 'Unsupervised Learning'}\n",
      "   Length: 194 characters\n",
      "   Content preview:\n",
      "----------------------------------------\n",
      "### Unsupervised Learning  \n",
      "Unsupervised learning algorithms work with unlabeled data. The system tries to learn without a teacher, finding hidden patterns or intrinsic structures in input data.\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š SUMMARY STATISTICS:\n",
      "   Total chunks: 2\n",
      "   Average chunk size: 703.5 characters\n",
      "   Largest chunk: 287 characters (chunk #0)\n",
      "   Smallest chunk: 172 characters (chunk #5)\n",
      "\n",
      "âœ… Chunks are now stored in the 'chunks' variable for use in the next step.\n",
      "   You can access individual chunks with chunks[0], chunks[1], etc.\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your markdown file\n",
    "markdown_file_path = \"../../assets/sample.md\"\n",
    "\n",
    "# Chunk the markdown file\n",
    "lc_chunks = chunk_markdown_file(markdown_file_path)\n",
    "if lc_chunks:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ðŸ“Š SUMMARY STATISTICS:\")\n",
    "    print(f\"   Total chunks: {len(chunks)}\")\n",
    "    \n",
    "    total_chars = sum(chunk['length'] for chunk in lc_chunks)\n",
    "    avg_chunk_size = total_chars / len(chunks) if chunks else 0\n",
    "    print(f\"   Average chunk size: {avg_chunk_size:.1f} characters\")\n",
    "    \n",
    "    max_chunk = max(lc_chunks, key=lambda x: x['length'])\n",
    "    min_chunk = min(lc_chunks, key=lambda x: x['length'])\n",
    "    print(f\"   Largest chunk: {max_chunk['length']} characters (chunk #{max_chunk['index']})\")\n",
    "    print(f\"   Smallest chunk: {min_chunk['length']} characters (chunk #{min_chunk['index']})\")\n",
    "\n",
    "# The chunks list is now available for use in the next notebook\n",
    "print(f\"\\nâœ… Chunks are now stored in the 'chunks' variable for use in the next step.\")\n",
    "print(f\"   You can access individual chunks with chunks[0], chunks[1], etc.\")\n",
    "\n",
    "# If going to use the LangChain chunks in the next step, set the chunks variable to the chunk text\n",
    "chunks = [item[\"content\"] for item in lc_chunks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d806d",
   "metadata": {},
   "source": [
    "## Step 2: Create Embeddings for Semantic Searches\n",
    "\n",
    "In this step you'll use AzureOpenAI to create the embeddings for the chunks you created above - you'll need to decide which chunking technique you like best.\n",
    "\n",
    "The code below will utilize the older text-embedding-ada-002 model for creating the embeddings. In Step 4, you'll get to compare the embeddings from OpenAI and see how they can differ in a semantic search.\n",
    "\n",
    "First you'll need to install the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "561eacdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a5505b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Create a token provider that returns a fresh bearer token on each call\n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(),\n",
    "    \"https://cognitiveservices.azure.com/.default\",\n",
    ")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_ad_token_provider=token_provider,\n",
    "    api_version=\"2024-02-01\",\n",
    ")\n",
    "\n",
    "def embed_chunks(text_chunks: list[str], model: str) -> list[list[float]]:\n",
    "    # model is your Azure deployment name, e.g. \"embeddings-prod\"\n",
    "    response = client.embeddings.create(\n",
    "        model=model,          # deployment name, not the base model id[web:42]\n",
    "        input=text_chunks,    # list of chunk strings\n",
    "    )\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eaf80c",
   "metadata": {},
   "source": [
    "Next you use the above utility to create embeddings of the chunks (created earlier) and take a look at a few of the returned vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c176c2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First embedding:\n",
      "[-0.02100842073559761, -0.003979153465479612, -0.0002070511254714802, -0.02178790792822838, -0.01785275712609291, 0.007222823332995176, 0.003259385470300913, 0.005214388482272625, -0.025019006803631783, -0.015313140116631985, 0.014747384004294872, 0.02328401990234852, -0.032663002610206604, -0.007373692002147436, -0.005714139901101589, -0.004095447715371847, 0.02549675665795803, -0.003567408537492156, 0.006983948405832052, -0.008863517083227634, -0.025157302618026733, 0.008938951417803764, 0.010221332311630249, -0.04174024984240532, -0.0010246477322652936, -0.01949973776936531, 0.020794691517949104, -0.037867963314056396, 0.0038817175664007664, -0.0025867640506476164, 0.009278405457735062, -0.021586749702692032, -0.001859138486906886, 0.0004050658899359405, -0.010868809185922146, -0.004717779811471701, 0.02535845898091793, -0.0016438367310911417, -0.0010136469500139356, 0.001255664974451065, 0.018393369391560555, 0.01457137055695057, -0.009196684695780277, -0.023937782272696495, 0.0035422637593001127, 0.026502544060349464, 0.00825375784188509, -0.021184435114264488, -0.012157476507127285, 0.01785275712609291, 0.016708672046661377, 0.027382610365748405, -0.0013350279768928885, -0.001719270832836628, -0.013942752964794636, 0.015803461894392967, -0.01374159473925829, 0.013967896811664104, 0.03075200505554676, -0.012264342047274113, -0.005934156011790037, 0.009643003344535828, -0.014822818338871002, -0.01387989055365324, -0.00431860750541091, 0.006958804093301296, -0.01830536313354969, 0.03927606716752052, 0.015841180458664894, -0.005997017957270145, 0.008989240974187851, 0.031129175797104836, -0.01140942145138979, -0.015061693266034126, 0.0298467930406332, 0.0005795074393972754, -0.018519094213843346, 0.0021074425894767046, -0.014521081000566483, 0.015614877454936504, -0.010259049944579601, -0.011440851725637913, 0.003969724290072918, -0.005824148189276457, 0.008492632769048214, -0.006408763118088245, -0.007342260796576738, 0.00017316467710770667, 0.002802065806463361, 0.009787585586309433, -0.008524063043296337, -0.012352348305284977, 0.013314134441316128, 0.015187416225671768, -0.012792381457984447, 0.033090464770793915, -0.005038375034928322, 0.018431086093187332, -0.002266168827190995, -0.03754108026623726, -0.013477575033903122, 0.012000322341918945, -0.015099409967660904, -0.00931612215936184, -0.00998874381184578, 0.005745570641011, -0.0077382903546094894, 0.0014002470998093486, 0.052904512733221054, 0.030374832451343536, -0.006383618339896202, 0.027960939332842827, 0.010981960222125053, -0.031581781804561615, -0.007191392593085766, 0.0027549194637686014, 0.0051798140630126, -0.023309163749217987, -0.015602304600179195, -0.003976010251790285, 0.01861967146396637, 0.012333489954471588, 0.006613064091652632, 0.007109672296792269, 0.008209753781557083, 0.006179317366331816, -0.006402476690709591, -0.01890883594751358, -0.00611016945913434, -0.016457226127386093, 0.007335974834859371, 0.011484854854643345, -0.010252763517200947, 0.023196013644337654, -0.02027922496199608, 0.004189740400761366, -0.007027952000498772, 0.003969724290072918, -0.024402959272265434, 0.009699579328298569, 0.0027863504365086555, -0.015325712971389294, 0.001783704268746078, -0.011176832020282745, -0.02115928940474987, 0.01753844879567623, 0.013037541881203651, 0.007895445451140404, 0.005783287808299065, 0.019084850326180458, 0.0006180102936923504, -0.020782118663191795, -0.03522776439785957, 0.008882375434041023, 0.023183440789580345, 0.01058593112975359, -0.0009248545975424349, 4.501004877965897e-05, 0.009932167828083038, 0.0033631075639277697, 0.011516286060214043, -0.004101734142750502, -0.008957809768617153, 0.00362398405559361, 0.007109672296792269, 0.019298579543828964, 0.006455909460783005, 0.0013633158523589373, 0.005600988399237394, -0.012886674143373966, 0.008172037079930305, 0.009768727235496044, -0.015992047265172005, 0.009982457384467125, 0.00818460900336504, 0.003234240924939513, 0.01816706731915474, -0.00724796811118722, -0.03090287186205387, 0.01606748253107071, -0.0083354776725173, 0.012924390845000744, -0.0052081020548939705, 0.015049120411276817, -0.009052102454006672, 0.005547556094825268, 0.0012108759256079793, -0.007832583039999008, -0.0006655495380982757, -0.006053593475371599, 0.009687007404863834, 0.013703878037631512, 0.00029289681697264314, -0.019550027325749397, -0.6557745337486267, -0.03205953165888786, 0.026351677253842354, 0.003664844436571002, 0.011007105000317097, 0.007411409169435501, 0.0013130262959748507, -0.01245292741805315, 0.024239519611001015, 0.02055581659078598, -0.011088825762271881, 0.009045816026628017, 0.005478407721966505, -0.010862522758543491, 0.015828607603907585, -0.02431495301425457, 0.014093620702624321, -0.04578855261206627, -0.019286006689071655, 0.007386264391243458, -0.02567276917397976, 0.023170867934823036, -0.026904860511422157, -0.00012434854579623789, -0.02687971666455269, 0.0014073190977796912, 0.00675764586776495, 0.022303374484181404, 0.007229109760373831, 0.010227618739008904, -0.019273435696959496, 0.0007386264041997492, 0.020681539550423622, -0.017903046682476997, 0.03321618586778641, 0.0030487985350191593, 3.6464767617871985e-05, 0.027684347704052925, 0.008090316317975521, 0.03271329402923584, -0.018066488206386566, -0.03381966054439545, -0.005654421169310808, 0.0038691451773047447, 0.008668645285069942, -0.013465003110468388, 0.02162446826696396, 0.02685457095503807, -0.012270628474652767, -0.03047541156411171, 0.0009940025629475713, -0.005909011233597994, -0.01830536313354969, -0.007417695131152868, -0.00991330947726965, -0.0026087656151503325, 0.004852932877838612, -0.002013149904087186, 0.018192211166024208, -0.028740426525473595, 0.017048126086592674, -0.0021938777063041925, -0.022718263790011406, -0.009712152183055878, -0.02863984741270542, 0.016155488789081573, 0.02266797423362732, -0.010900240391492844, -0.015552015043795109, -0.030802292749285698, 0.02310800552368164, 0.0058618648909032345, -0.008247471414506435, -0.009435559622943401, 0.0006325470749288797, 0.021121572703123093, 0.010655079036951065, -0.008894948288798332, -0.013867318630218506, -0.015816034749150276, 0.0005146811599843204, -0.008938951417803764, 0.0033316765911877155, -0.029645636677742004, 0.030726859346032143, -0.009492135606706142, 0.005396687425673008, -0.004258888773620129, -0.010906526818871498, 0.013276416808366776, 0.03590667247772217, 0.030223965644836426, -0.01905970461666584, -0.06411906331777573, -0.0208575539290905, 0.005833577364683151, -0.0020194360986351967, -0.017073271796107292, -0.01134027261286974, -0.019738612696528435, 0.005336968693882227, 0.009479562751948833, 0.0034291124902665615, 0.019122567027807236, -0.01147228293120861, -0.006569060496985912, 0.01276095025241375, 0.004626630339771509, 0.02489328198134899, -0.02504415065050125, 0.001914142514578998, 0.016004620119929314, -0.008115461096167564, 0.037918251007795334, -0.004956654738634825, -0.023761769756674767, -0.019688323140144348, 0.0028727853205055, 0.019462021067738533, -0.031305186450481415, 0.03947722539305687, -0.0038502865936607122, 0.013326706364750862, -0.00831661932170391, -0.012276913970708847, -0.009586428292095661, -0.013465003110468388, -0.01431992370635271, -0.0060190195217728615, -0.006047307513654232, 0.010963101871311665, -0.011805450543761253, -0.003300245851278305, -0.010309339500963688, 0.030223965644836426, -0.016746390610933304, 0.006166744977235794, 0.0029670780058950186, -0.0029057879000902176, -0.005965587217360735, -0.01156657561659813, -0.006141600199043751, 0.01589146815240383, 0.006921086926013231, -0.02773463726043701, -0.0073988367803394794, 0.005368399899452925, 0.003708847565576434, 0.000821132562123239, -0.008385767228901386, -0.019260862842202187, -8.068904571700841e-05, -0.002349460730329156, 0.04339980334043503, -0.005490980111062527, -0.0041803112253546715, -0.018041344359517097, -0.015401146374642849, -0.03122975490987301, -0.03437284380197525, -0.02941933274269104, 0.030676569789648056, -0.015589732676744461, 0.0185945276170969, -0.007078241556882858, -0.01081851962953806, -0.0040828753262758255, 0.03120460920035839, -0.0333419106900692, -0.033668793737888336, 0.035353489220142365, -0.016721244901418686, -0.0009806444868445396, 0.027131162583827972, -0.012434069067239761, 0.009856734424829483, -0.01800362579524517, 0.009944740682840347, -0.019550027325749397, -0.011899743229150772, -0.014370213262736797, 0.014772528782486916, 0.023786913603544235, -1.3468637007463258e-05, 0.010975674726068974, 0.004632916301488876, 0.037616513669490814, -0.0013641015393659472, -0.022881703451275826, 0.023070288822054863, 0.01486053504049778, 0.03779252991080284, -0.0009334980859421194, -0.006455909460783005, 0.0030959448777139187, 0.016469797119498253, -0.005534983705729246, -0.009674434550106525, 0.003012652974575758, 0.02100842073559761, 0.01665838249027729, 0.005934156011790037, 0.01281752623617649, 0.008838372305035591, -0.002649625763297081, -0.01537600252777338, 0.002522330731153488, -0.0295702014118433, 0.005028945859521627, -0.003803140250965953, 0.025421321392059326, -0.014533653855323792, 0.005946728400886059, -0.034750014543533325, 0.012591223232448101, 0.027835216373205185, -0.008140605874359608, 0.01937401480972767, -0.05209987983107567, -0.01222662441432476, 0.0010356485145166516, 0.017676744610071182, 0.007732004392892122, -0.02941933274269104, -0.0023101719561964273, 0.031104030087590218, 0.011522572487592697, 0.012446640990674496, -0.017337290570139885, -0.05421203747391701, 0.013653588481247425, 0.006229606922715902, 0.006650780793279409, 0.014508509077131748, 0.014772528782486916, 0.0012368063908070326, 0.004485191311687231, -0.020643822848796844, 0.04244430363178253, -0.024993861094117165, -0.01636921800673008, -0.009385270066559315, 0.008781797252595425, -0.02863984741270542, 0.016004620119929314, -0.004746067803353071, 0.037289634346961975, -0.00510123698040843, -0.01830536313354969, 0.01574060134589672, -0.039854396134614944, -0.006242179311811924, -0.01800362579524517, -0.0007362691103480756, -0.017148705199360847, -0.040307000279426575, 0.006317613180726767, -0.014194199815392494, 0.03288930654525757, 0.018669961020350456, 0.008605783805251122, 0.022441670298576355, 0.014458219520747662, -0.02219022437930107, -0.0002995758841279894, -0.014282206073403358, 0.02239138074219227, -0.012584936805069447, 0.010787089355289936, -0.007549704983830452, -0.017601311206817627, -0.024490967392921448, -0.009888164699077606, -0.023497749119997025, 0.03500146418809891, -0.017211567610502243, 0.02117186225950718, 0.016105199232697487, 0.013414713554084301, 0.029670780524611473, -0.02775978110730648, -0.02416408434510231, 0.006232749670743942, 0.02338459901511669, 0.00831661932170391, -0.0063553303480148315, -0.027986083179712296, 0.015602304600179195, -0.0246292632073164, -0.023019999265670776, -0.0032216685358434916, 0.02640196681022644, -0.010523069649934769, -5.294144284562208e-05, -0.006119598634541035, -0.006345901172608137, 0.041187066584825516, 0.01507426518946886, -0.00010087358532473445, -0.02103356644511223, -0.006383618339896202, 0.013037541881203651, 0.0012721661478281021, -0.019612889736890793, 0.035780951380729675, 0.005990731995552778, 0.006675925571471453, 0.0006011162186041474, -0.009404128417372704, -0.02087012492120266, -0.002533331513404846, 0.013276416808366776, 0.028111808001995087, -0.004500906448811293, 0.010290480218827724, -0.0039257206954061985, 0.015828607603907585, 0.0035705515183508396, 0.005632419604808092, 0.013791884295642376, 0.00359883951023221, -0.0240132175385952, 0.011283697560429573, 0.031305186450481415, 0.06582890450954437, 0.009171539917588234, -0.014194199815392494, 0.0157783180475235, -0.027860360220074654, -0.0024877567775547504, -0.034875739365816116, 0.00458262674510479, 0.006518770940601826, 0.011585433967411518, -0.028715280815958977, 0.020945560187101364, 0.02220279537141323, 0.01919800043106079, 0.01140942145138979, 0.027533479034900665, 0.021234724670648575, -0.008731507696211338, -0.010365914553403854, -0.013100404292345047, 0.014483364298939705, 0.01713613234460354, 0.02117186225950718, -0.013540436513721943, 0.006575346924364567, -0.020228935405611992, 0.019876908510923386, 0.017890475690364838, -0.032160110771656036, -0.019851764664053917, 0.015199989080429077, 0.02534588798880577, -0.004805786535143852, -0.01170487143099308, -0.02356061153113842, 0.04935910552740097, 0.00856806617230177, 0.029796505346894264, 0.011503714136779308, -0.03168236091732979, 0.041036199778318405, 0.04040757939219475, 0.00010765087063191459, 0.0026574835646897554, -0.017488159239292145, -0.0211341455578804, 0.006619350053369999, 0.024352669715881348, -0.005767572205513716, -0.03633413463830948, 0.011013391427695751, 0.013351851142942905, -0.03510204330086708, 0.01756359450519085, 0.004827788099646568, 0.02295713871717453, 0.016432080417871475, 0.0006643708911724389, 0.0009413558291271329, -0.027206597849726677, -0.01620577834546566, -0.02550932765007019, 0.006675925571471453, 0.002753347856923938, 0.005654421169310808, -0.01089395396411419, -0.001739701023325324, 0.009636717848479748, -0.03276358172297478, 0.006921086926013231, 0.0021891631186008453, -0.045386236160993576, -0.018041344359517097, -0.012584936805069447, -3.307612496428192e-05, 0.02625109814107418, 0.010447635315358639, -0.006675925571471453, -0.030274255201220512, 0.03228583186864853, -0.014370213262736797, -0.023623473942279816, -0.014294778928160667, -0.026351677253842354, -0.0036962751764804125, -0.004966083914041519, -0.009599000215530396, -0.009705865755677223, -0.0018497091950848699, 0.005770715419203043, -0.008209753781557083, -0.02836325578391552, 0.005456406157463789, 0.02188848704099655, 0.010761944577097893, 0.013754166662693024, 0.01214490458369255, 0.005563271231949329, 0.017903046682476997, -0.022240513935685158, 0.010441348887979984, -0.011660868301987648, -0.03200924023985863, -0.01726185716688633, 0.006361616775393486, -0.0166709553450346, 0.00018544237536843866, 0.014231916517019272, 0.01206947024911642, -0.03120460920035839, 0.010611075907945633, 0.016142915934324265, 0.041363079100847244, -0.006072452291846275, 0.02760891243815422, 0.012698088772594929, 0.028463833034038544, 0.002424894832074642, 0.024868138134479523, -0.014257061295211315, -0.0016548375133424997, -0.03286416083574295, 0.025320742279291153, 0.014634232968091965, -0.00998874381184578, 0.0016941261710599065, 0.019323725253343582, -0.0160549096763134, -0.03419683128595352, -0.005151526536792517, -0.009328695014119148, 0.023837203159928322, -0.010680223815143108, -0.019122567027807236, -0.01723671145737171, -0.016130343079566956, -0.017437869682908058, 0.02037980407476425, -0.01844365894794464, -0.007989738136529922, 0.01320098340511322, 0.01042877696454525, 0.004390898626297712, -0.01592918671667576, 0.01785275712609291, -0.017035555094480515, -0.0010592216858640313, 0.004695778246968985, -0.016570376232266426, 0.03195895254611969, -0.006481054238975048, 0.043726686388254166, -0.030525701120495796, -0.014294778928160667, 0.006132171023637056, 0.0033693937584757805, 0.004768069367855787, -0.012716947123408318, 0.04219285771250725, 0.03246184438467026, 0.02657797932624817, -0.012025467120110989, 0.02058096043765545, -0.008964096195995808, -0.01740015298128128, -0.021712474524974823, 0.006914800498634577, 0.01918542943894863, 0.008128033950924873, 0.006283039227128029, 0.007298257667571306, -0.008825800381600857, 0.02567276917397976, -0.018493948504328728, 0.02310800552368164, 0.002943504834547639, 0.01389246340841055, -0.006732501555234194, -0.02009063959121704, -0.018581954762339592, -0.006000161170959473, 0.03045026771724224, 0.010636220686137676, 0.042544882744550705, -0.015816034749150276, 0.004639202728867531, 0.02012835629284382, 0.017463015392422676, 0.008423483930528164, 0.01449593622237444, 0.012019180692732334, 0.002462611999362707, -0.01035334262996912, 0.029193030670285225, 0.012861529365181923, -0.006088167428970337, 0.00686451094225049, -0.035806093364953995, 0.016306357458233833, 0.02026665210723877, 0.011535144411027431, 0.01592918671667576, 0.01874539628624916, -0.0033536783885210752, -0.021272441372275352, 0.016771534457802773, -0.017450442537665367, -0.00996988546103239, 0.012999825179576874, -0.02148617058992386, 0.002690486144274473, -0.029519911855459213, -0.00831661932170391, 0.02025407925248146, 0.014646804891526699, 0.022843986749649048, 0.003539120778441429, 0.01081851962953806, -0.03932635858654976, -0.01680925115942955, 0.024654407054185867, -0.016884686425328255, 0.04759897291660309, -0.0032939596567302942, 0.04138822481036186, 0.007882872596383095, 0.01861967146396637, 0.00931612215936184, 0.022454243153333664, 0.004500906448811293, 0.013364423997700214, 0.02745804376900196, 0.0012745234416797757, -0.0030377975199371576, -0.02672884799540043, -0.01830536313354969, -0.019273435696959496, 0.0032719578593969345, -0.03105374053120613, 0.002263025613501668, 0.017048126086592674, 0.0010513640008866787, -0.01635664701461792, -0.006499912589788437, -0.010001315735280514, 0.04334951192140579, 0.006455909460783005, 0.006946231704205275, -0.007329688873142004, 0.0024217518512159586, 0.010309339500963688, 0.015287995338439941, -0.010466493666172028, 0.013427285477519035, -0.015866324305534363, 0.025584762915968895, 0.0014780386118218303, 0.001463108928874135, -0.01431992370635271, -0.011214549653232098, -0.004497763700783253, 0.029067307710647583, -0.0036208410747349262, 0.01711098849773407, 0.024704696610569954, 0.01208832859992981, -0.006921086926013231, -0.0023793200962245464, -0.0025411893147975206, 0.02899187244474888, -0.015979476273059845, -0.009328695014119148, -0.008216040208935738, 0.009963599033653736, 0.002269311808049679, -0.019876908510923386, -0.019700895994901657, -0.007367405574768782, -0.01395532488822937, -4.0639188227942213e-05, 0.01320098340511322, 0.02353546768426895, -0.03945207968354225, 0.015438864007592201, 0.005572700873017311, -0.026779137551784515, 0.011390562169253826, -0.01079966127872467, 0.02234109304845333, -0.018720250576734543, -0.004922080785036087, -0.012848956510424614, -0.021071283146739006, 0.008725221268832684, 0.016281211748719215, -0.01079966127872467, -0.0004683205916080624, 0.02040494792163372, -0.007977165281772614, 0.00846748799085617, -0.006088167428970337, 0.0015165414661169052, -0.010975674726068974, 0.00551612488925457, -0.011126542463898659, -0.011648296378552914, 0.013188410550355911, -0.02838839963078499, -0.0034574002493172884, -0.026753991842269897, 0.002833496779203415, 0.006405619904398918, -0.022768553346395493, -0.016469797119498253, -0.004381468985229731, 0.016180632635951042, 0.004670633468776941, -0.0006172245484776795, 0.005795860197395086, 0.0012226625112816691, -0.006996520794928074, -0.010177329182624817, 0.017337290570139885, -0.032210398465394974, 0.008172037079930305, 0.00701537961140275, 0.015715455636382103, -0.002621338004246354, -0.04181568697094917, -0.026502544060349464, -0.005940442439168692, 0.016608092933893204, 0.002637053607031703, 0.007386264391243458, 0.012207766063511372, 0.01878311298787594, -0.010089322924613953, -0.015237705782055855, -0.005390401463955641, -0.0004714636888820678, 0.010007602162659168, 0.00839205365628004, 0.02715630829334259, 0.007235395722091198, -0.02506929449737072, -0.0073988367803394794, -0.008876089937984943, -0.035630080848932266, -0.03811940923333168, -0.0009146395605057478, -0.03696275129914284, 0.04168996214866638, 0.006487340200692415, -0.01802877150475979, -0.023007428273558617, -0.0014693951234221458, -0.041212212294340134, 0.006047307513654232, 0.008304046466946602, 0.0006191889406181872, -0.009473277255892754, 0.02836325578391552, -0.01079966127872467, 0.01260379608720541, -0.007034237962216139, 0.023233730345964432, -0.004827788099646568, 0.01606748253107071, -0.02909245155751705, 0.0185945276170969, -0.0030000805854797363, -0.012364921160042286, -0.005182957276701927, -0.01937401480972767, -0.020216362550854683, -0.0336185023188591, -0.0037497077137231827, 0.034599147737026215, -0.008894948288798332, -0.007669142447412014, 0.01623092219233513, 0.015828607603907585, 0.015099409967660904, -0.0034008247312158346, 0.013351851142942905, -0.007411409169435501, -0.008869803510606289, -3.381278656888753e-05, -0.014156483113765717, -0.011541430838406086, 0.0004455331654753536, -0.005358970258384943, -0.01387989055365324, 0.017148705199360847, -0.002915217075496912, -0.008461201563477516, -0.007838869467377663, -0.0025867640506476164, 0.023321736603975296, 0.010001315735280514, -0.0025474755093455315, 0.013703878037631512, -0.010378487408161163, -0.0036428426392376423, -0.015539443120360374, 0.0010796518763527274, -0.01081851962953806, -0.01802877150475979, 0.010862522758543491, -0.01634407415986061, 0.004057730548083782, 0.0023793200962245464, 0.021373020485043526, 0.0008690647082403302, -0.016897257417440414, 0.006506198551505804, 0.013716449961066246, -0.02612537331879139, 0.009573855437338352, -0.02402578853070736, -0.00796459335833788, -0.0055821300484240055, 0.012358634732663631, -0.007782293949276209, -0.03193380683660507, -0.016444653272628784, 0.01043506246060133, -0.024704696610569954, 0.007562277372926474, 0.029469622299075127, 0.00036852742778137326, -0.02313315123319626, 0.0036428426392376423, -0.0021090141963213682, -0.019776329398155212, 0.008492632769048214, 0.1882837414741516, -0.040910474956035614, -0.0028743569273501635, 0.002789493417367339, -0.020656395703554153, 0.005019516684114933, 0.023497749119997025, 0.0060567366890609264, 0.004692635033279657, 0.007423981558531523, 0.011327700689435005, 0.01938658580183983, -0.02760891243815422, 0.005091807805001736, 0.018217356875538826, -0.030123386532068253, -0.014420502819120884, -0.029620490968227386, 0.005233246833086014, 0.01530056819319725, 0.011723729781806469, 0.012949535623192787, -0.03349278122186661, -0.031028596684336662, 0.030098240822553635, 0.013829600997269154, -0.0022598826326429844, 0.022315947338938713, 0.011170546524226665, 0.011541430838406086, -0.02037980407476425, 0.002835068153217435, 0.017349863424897194, -0.013012397103011608, -0.02926846593618393, -0.006122741848230362, 0.009171539917588234, -0.0027549194637686014, 0.023472605273127556, 0.021058710291981697, 0.025433894246816635, -0.0038754313718527555, 0.01381702907383442, -0.00458262674510479, 0.02012835629284382, 0.03258756920695305, -0.01507426518946886, -0.01936144195497036, 0.003910005558282137, 0.011484854854643345, -0.020593533292412758, -0.009246974252164364, 0.013691305182874203, 0.0149233965203166, -0.019273435696959496, 0.017324717715382576, -0.011465996503829956, 0.021938776597380638, 0.017651600763201714, -0.001468609319999814, -0.012874101288616657, -0.00781372468918562, 0.005654421169310808, 0.026326531544327736, -0.0021357303485274315, 0.007109672296792269, -0.011252266354858875, 0.011730016209185123, 0.002517616143450141, 0.0021844482980668545, -0.01499883085489273, -0.005292965564876795, -0.009649289771914482, -0.02773463726043701, -0.01981404609978199, -0.027960939332842827, 0.03122975490987301, 0.01164200995117426, 0.0016053338767960668, 0.014470791444182396, -0.003903719363734126, -0.029821649193763733, -0.03379451483488083, -0.02417665719985962, -0.021385593339800835, -0.01591661386191845, 0.028539268299937248, 0.0006789076724089682, 0.010070464573800564, -0.03253728151321411, -0.004265174735337496, -0.028564412146806717, -0.003072371706366539, -0.025999650359153748, 0.010416204109787941, 0.02281884290277958, 0.019600316882133484, 0.010541928000748158, 0.010120753198862076, 0.010711655020713806, -0.027583768591284752, 0.045713119208812714, 0.02025407925248146, 0.017500732094049454, 0.004042015410959721, 0.011698585003614426, -0.009856734424829483, 0.01079966127872467, 0.00017267357907257974, 0.000356151518644765, -0.030073096975684166, -0.04146365821361542, 0.012138618156313896, -0.008819513954222202, -0.012949535623192787, 0.007939448580145836, -0.01095681544393301, -0.008121747523546219, 0.009812730364501476, -0.009649289771914482, -0.0192105732858181, 0.0020791548304259777, -0.020178645849227905, 0.011095112189650536, -0.005607274826616049, -0.005421832203865051, -0.0025490468833595514, 0.008304046466946602, -0.012346061877906322, -0.0080274548381567, 0.008938951417803764, -0.03583123907446861, -0.008442343212664127, 0.006421335507184267, -0.013703878037631512, -0.00048050007899291813, 0.0128992460668087, 0.0014285349752753973, -0.008429770357906818, 0.005676422733813524, -0.021875914186239243, 0.028287820518016815, -0.022466816008090973, 0.015099409967660904, 0.004372039809823036, 0.0021561605390161276, 0.010441348887979984, 0.023158295080065727, -0.024428104981780052, -0.01095681544393301, 0.006220177281647921, -0.005487837363034487, 0.009856734424829483, -0.011170546524226665, 0.020367231220006943, 0.009548710659146309, -0.024050934240221977, 0.009378984570503235, 0.02763405814766884, -0.004177168011665344, -0.0404830127954483, 0.016733817756175995, 0.0012595937587320805, -0.012710660696029663, -0.005079235415905714, 0.0009075675625354052, -0.15549500286579132, -0.022328520193696022, 0.01740015298128128, -0.025157302618026733, 0.03449856862425804, 0.014156483113765717, 0.001679982291534543, 0.011308842338621616, -0.004815215710550547, -0.0015110410749912262, 0.004155166447162628, -0.005704710725694895, 0.005475264973938465, -0.008574352599680424, -0.004057730548083782, 0.02474241331219673, -0.009812730364501476, 0.020983276888728142, 0.030123386532068253, -0.001745987101458013, 0.021083856001496315, -0.03208467364311218, 0.00626418087631464, -0.005789573770016432, 0.024679552763700485, -0.024541256949305534, -0.001947144977748394, 0.015338284894824028, -0.0008517777314409614, -0.02969592623412609, 0.0005728283431380987, -0.014508509077131748, 0.013540436513721943, -0.008656073361635208, 0.034423135221004486, -0.005107523407787085, 0.006789077073335648, -0.017375007271766663, -0.01787790283560753, -0.004466332495212555, 0.022856559604406357, 0.02313315123319626, 0.023019999265670776, -1.8207829270977527e-05, 0.028740426525473595, 0.015828607603907585, 0.017450442537665367, -0.0208575539290905, 0.009378984570503235, -0.013691305182874203, 0.01953745447099209, -0.009014385752379894, -0.0017601310973986983, -0.01678410731256008, 0.026603123173117638, -0.000296432786853984, -0.012396351434290409, -0.010793374851346016, -0.0027863504365086555, -0.010378487408161163, 0.01027790829539299, -0.006851939018815756, 0.018707679584622383, 0.00102229043841362, -0.006462195422500372, -0.005824148189276457, 0.000739805109333247, 0.019160283729434013, -0.013465003110468388, 0.0027219168841838837, 0.020316941663622856, 0.009290977381169796, -0.02011578343808651, -0.006782790645956993, -0.012044325470924377, 0.0024327526334673166, 0.006279896013438702, 0.014948541298508644, -0.0034888312220573425, -0.01305011473596096, -0.001354672247543931, 0.0172870010137558, -0.0027124877087771893, 0.012138618156313896, 0.0019094279268756509, -0.012628940865397453, 0.005201816093176603, -0.02416408434510231, -0.015061693266034126, -0.022328520193696022, 0.025132156908512115, -0.02941933274269104, -0.03379451483488083, -0.0038471436128020287, 0.006374189164489508, 0.0198266189545393, -0.012182621285319328, -0.01499883085489273, 0.01185574010014534, -0.0023133151698857546, 0.02413894049823284, 0.004004298243671656, 0.0009248545975424349, -0.0019832905381917953, 0.014772528782486916, 0.005082378629595041, -0.0011543002910912037, -0.016394363716244698, 0.030399978160858154, -0.01815449446439743, -0.0033033888321369886, -0.010868809185922146, 0.008618355728685856, 0.04460674896836281, -0.00908353365957737, 0.008417198434472084, -0.0009107106598094106, -0.024654407054185867, 0.0011951604392379522, 0.0026040510274469852, 0.021549033001065254, -0.029193030670285225, -0.0004109591827727854, 0.020819835364818573, -0.011440851725637913, -0.021071283146739006, -0.10369686782360077, -0.04410385712981224, -0.014948541298508644, 0.013942752964794636, -0.02685457095503807, 0.026225952431559563, -0.02594936080276966, 0.013351851142942905, -0.027810070663690567, 0.027960939332842827, -0.004588913172483444, -0.019474593922495842, -0.014143910259008408, -0.02388749271631241, -0.024981288239359856, -0.019323725253343582, -0.015803461894392967, -0.0027329178992658854, -0.02866499125957489, 0.009617859497666359, -0.0023416029289364815, -0.0011456566862761974, 7.951038423925638e-05, 0.007449126336723566, -0.0005543627194128931, 0.02207707241177559, -0.020078066736459732, 0.009058388881385326, 0.025270452722907066, 0.023824630305171013, 0.01965060643851757, -0.03625870123505592, -0.016897257417440414, -0.02205192856490612, 0.017664171755313873, 0.0019817189313471317, -0.0074805570766329765, 0.00031038024462759495, 0.014659377746284008, -0.003089658683165908, -0.0003200059582013637, -0.0036679874174296856, 0.029796505346894264, 0.003246813314035535, 0.0029136454686522484, -0.00846748799085617, -0.01724928431212902, 0.024076078087091446, -0.011315127834677696, -0.037289634346961975, -0.029670780524611473, -0.015099409967660904, -0.04845389351248741, 0.014810245484113693, -0.011346559040248394, 0.007499415427446365, 0.016306357458233833, -0.02099584974348545, -0.010057891719043255, -0.017475586384534836, -0.0037402785383164883, -0.00768800126388669, -0.03648500144481659, 0.02625109814107418, 0.006201318930834532, 0.0053275395184755325, -0.003507689805701375, 0.005217531230300665, 0.0018072774400934577, -0.010805947706103325, 0.014810245484113693, 0.01861967146396637, -0.011811736971139908, 0.02685457095503807, -0.035051751881837845, 0.016394363716244698, -0.013477575033903122, -0.010296766646206379, 0.010749371722340584, 0.004293462727218866, -0.017639027908444405, -0.012691802345216274, -0.014194199815392494, -0.024239519611001015, 0.011912315152585506, 0.004114306531846523, -0.03288930654525757, -0.0023101719561964273, 0.0005292179994285107, -0.0006172245484776795, -0.0031698073726147413, 0.013112976215779781, 0.024666979908943176, 0.009353839792311192, -0.007178820203989744, -0.012767236679792404, 0.002508186735212803, -0.029067307710647583, 0.01724928431212902, 0.02024150639772415, 0.001288667437620461, -0.019286006689071655, -0.0546143539249897, 0.0013373852707445621, -0.013150693848729134, 0.004469475708901882, 0.01638179086148739, -0.012044325470924377, 0.01636921800673008, -0.037415359169244766, 0.012214052490890026, 0.007442839909344912, -0.03477516025304794, 0.002289741998538375, -0.004466332495212555, -0.0009185684029944241, -0.012484358623623848, -0.003988582640886307, 0.01878311298787594, 0.0037339923437684774, 0.00012316988431848586, -0.007141103036701679, 0.012949535623192787, -0.0002096048992825672, 0.04694521054625511, 0.009712152183055878, 0.011208263225853443, 0.0057990034110844135, -0.010837377980351448, 0.017739607021212578, -0.02100842073559761, -0.001560544827952981, -0.006638208404183388, -0.00577700138092041, -0.009554997086524963, 0.006631922442466021, -0.029595347121357918, -0.026753991842269897, 0.017161278054118156, 0.03764165937900543, -0.0011511571938171983, 0.0632641389966011, -0.017613882198929787, -0.027684347704052925, 0.014835390262305737, 0.01652008667588234, -0.0257984921336174, -0.015476580709218979, -0.0010890810517594218, -0.002676342148333788, 0.02972107008099556, 0.006345901172608137, 0.030399978160858154, 0.0040828753262758255, -0.02310800552368164, -0.013829600997269154, -0.019700895994901657, -0.0017491301987320185, 0.01918542943894863, -0.013628443703055382, 0.00015047549095470458, -0.009096105583012104, 0.017500732094049454, 0.0016076911706477404, 0.005189243704080582, 0.005978159606456757, -0.015866324305534363, -0.00810288917273283, -0.009825303219258785, 0.03344248980283737, -0.0010191473411396146, -0.014533653855323792, -0.008115461096167564, -0.005135810934007168, -0.00499437190592289, -0.027659201994538307, 0.044204436242580414, 0.021071283146739006, -0.010422490537166595, 0.020618679001927376, -0.023925209417939186, 0.0048937927931547165, 0.013993041589856148, 5.102611976326443e-05, 0.007625139318406582, 0.013691305182874203, 0.042243145406246185, 0.012107186950743198, -0.004827788099646568, 0.0056418487802147865, 0.013238700106739998, -0.014470791444182396, 0.0008816370973363519, 0.009592714719474316, 0.0005091807688586414, 0.024666979908943176, 0.022152505815029144, -0.006990234833210707, -0.008291474543511868, -0.0036428426392376423, 0.014986258931457996, 0.03409625217318535, 0.03723934292793274, -0.014948541298508644, -0.0031588065903633833, -0.004268317949026823, -0.03195895254611969, 0.004777498543262482, -0.025773348286747932, -0.0160549096763134, -0.006330185569822788, 0.039376646280288696, 0.029343899339437485, 0.01828021928668022, -0.005104380194097757, -0.01638179086148739, -0.012440354563295841, 0.02191363088786602, 0.021863343194127083, 0.003476258832961321, -0.03585638478398323, 0.013565581291913986, -0.004453760106116533, -0.013502719812095165, 0.04355067014694214, -0.028136951848864555, 0.0076754288747906685, 0.04023156687617302, 0.013112976215779781, -0.026779137551784515, -0.0008627785136923194, -0.009561283513903618, -0.016570376232266426, -0.007493129465728998, -0.01861967146396637, 0.005299251526594162, -0.006462195422500372, -0.01951231062412262, 0.0067702182568609715, 0.02027922496199608, -0.005754999816417694, 0.07337231934070587, 0.0065124849788844585, 0.01006417814642191, 0.019172856584191322, 0.0008494203793816268, 0.00419602682814002, 0.014470791444182396, 0.004654917865991592, -0.020631249994039536, -0.01291181892156601, 0.003969724290072918, 0.009536138735711575, -0.009825303219258785, -0.019336296245455742, -0.010328197851777077, 0.013150693848729134, -0.013867318630218506, 0.0363592803478241, -0.003966581076383591, -0.017676744610071182, 0.012408924289047718, 0.015652593225240707, 0.018255073577165604, 0.004541766829788685, -0.016582949087023735, 0.0076754288747906685, 0.03002280741930008, 0.008580639027059078, -0.005968729965388775, -0.020769545808434486, 0.027860360220074654, -0.00039602949982509017, -0.033719081431627274, -0.01439535804092884, 0.007977165281772614, 0.0036396996583789587, 0.008234898559749126, -0.013779311440885067, -0.00026696629356592894, 0.007260540500283241, 0.005292965564876795, 0.017739607021212578, -0.032034385949373245, -0.04712122306227684, -0.02838839963078499, 0.009290977381169796, -0.03135547786951065, -0.028338110074400902, -0.012999825179576874]\n",
      "1536 dimensions\n",
      "\n",
      "First two embeddings:\n",
      "Embedding 0:\n",
      "[-0.02100842073559761, -0.003979153465479612, -0.0002070511254714802, -0.02178790792822838, -0.01785275712609291, 0.007222823332995176, 0.003259385470300913, 0.005214388482272625] ...\n",
      "dim: 1536\n",
      "Embedding 1:\n",
      "[-0.00475973729044199, 0.010285484604537487, 0.021068548783659935, -0.026005057618021965, -0.02736685425043106, 0.001700607710517943, 0.004278526175767183, 0.02197204902768135] ...\n",
      "dim: 1536\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings = embed_chunks(chunks, model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Single embedding (first item)\n",
    "print(\"First embedding:\")\n",
    "print(embeddings[0])  # a list[float]\n",
    "print(len(embeddings[0]), \"dimensions\")\n",
    "\n",
    "# First two embeddings\n",
    "print(\"\\nFirst two embeddings:\")\n",
    "for i, emb in enumerate(embeddings[:2]):  # slice to first 2[web:68][web:72]\n",
    "    print(f\"Embedding {i}:\")\n",
    "    print(emb[:8], \"...\")  # show just first few dims to keep output short\n",
    "    print(\"dim:\", len(emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e6c13",
   "metadata": {},
   "source": [
    "## Step 3: Load Azure AI Search Index\n",
    "\n",
    "Next step is the inserting of the chunks and embeddings into a vector database. In this step we'll use Azure AI Search as the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f660dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    "    HnswAlgorithmConfiguration,\n",
    ")\n",
    "\n",
    "search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "my_initials = os.getenv(\"MY_INITIALS\")\n",
    "index_name = f\"{my_initials.lower()}vectorindex\"\n",
    "embedding_dimension = 1536  # e.g. 1536 for ada-002\n",
    "\n",
    "# Create SearchClient for later use\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(search_key),\n",
    ")\n",
    "\n",
    "def ensure_chunk_vector_index() -> None:\n",
    "    \"\"\"\n",
    "    Ensure an Azure AI Search index exists for chunk text + embeddings.\n",
    "    If it does not exist, create it. If it exists, do nothing.\n",
    "    \"\"\"\n",
    "    if not my_initials:\n",
    "        raise ValueError(\"MY_INITIALS environment variable must be set in order to prevent index name collisions.\")\n",
    "\n",
    "    if not search_endpoint or not search_key:\n",
    "        raise ValueError(\"AZURE_SEARCH_ENDPOINT and AZURE_SEARCH_API_KEY must be set or passed in.\")\n",
    "\n",
    "    credential = AzureKeyCredential(search_key)\n",
    "    index_client = SearchIndexClient(endpoint=search_endpoint, credential=credential)\n",
    "\n",
    "    # Check if the index already exists\n",
    "    existing_names = list(index_client.list_index_names())\n",
    "    if index_name in existing_names:\n",
    "        print(f\"Index '{index_name}' already exists; skipping creation.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Index '{index_name}' does not exist; creating now...\")\n",
    "\n",
    "    fields = [\n",
    "        SimpleField(\n",
    "            name=\"id\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            key=True,\n",
    "            filterable=True,\n",
    "            sortable=True,\n",
    "            facetable=True,\n",
    "        ),\n",
    "        SearchableField(\n",
    "            name=\"content\",\n",
    "            type=SearchFieldDataType.String,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"contentVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=embedding_dimension,\n",
    "            vector_search_profile_name=\"chunk-vector-profile\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"chunk-hnsw-config\",\n",
    "                kind=\"hnsw\",\n",
    "            )\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"chunk-vector-profile\",\n",
    "                algorithm_configuration_name=\"chunk-hnsw-config\",\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "    )\n",
    "\n",
    "    result = index_client.create_index(index)  # only create because we know it doesn't exist\n",
    "    print(f\"Index '{result.name}' created.\")\n",
    "\n",
    "\n",
    "def upload_chunks_with_embeddings(\n",
    "    chunks: List[str],\n",
    "    embeddings: List[List[float]],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Upload chunks and their corresponding embeddings to the Azure AI Search index.\n",
    "    \"\"\"\n",
    "    if len(chunks) != len(embeddings):\n",
    "        raise ValueError(\"chunks and embeddings must have the same length\")\n",
    "\n",
    "    docs = []\n",
    "    for i, (text, vector) in enumerate(zip(chunks, embeddings)):\n",
    "        docs.append(\n",
    "            {\n",
    "                \"@search.action\": \"mergeOrUpload\",  # or \"upload\" if you only insert[web:85][web:88]\n",
    "                \"id\": str(i),\n",
    "                \"content\": text,\n",
    "                \"contentVector\": vector,  # must match index vector field name & dimensions[web:87][web:91]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Azure AI Search supports up to 1,000 docs per batch; keep it small for now\n",
    "    batch_size = 1000\n",
    "    for start in range(0, len(docs), batch_size):\n",
    "        batch = docs[start : start + batch_size]\n",
    "        result = search_client.upload_documents(documents=batch)\n",
    "        # Optional: check status per doc\n",
    "        succeeded = sum(1 for r in result if r.succeeded)\n",
    "        print(f\"Uploaded {succeeded}/{len(batch)} documents in batch starting at {start}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b02178",
   "metadata": {},
   "source": [
    "Next you can run the code that will ensure the index has been created and then load the chunks and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2cf9061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'jahvectorindex' does not exist; creating now...\n",
      "Index 'jahvectorindex' created.\n",
      "Uploaded 6/6 documents in batch starting at 0.\n"
     ]
    }
   ],
   "source": [
    "# first make sure the index exists\n",
    "ensure_chunk_vector_index()\n",
    "\n",
    "# chunks: list[str]  (your chunk strings from above - either Semantic Kernel or LangChain)\n",
    "# embeddings: list[list[float]] generated from Azure OpenAI for each chunk using text-embedding-ada-002\n",
    "upload_chunks_with_embeddings(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d061804f",
   "metadata": {},
   "source": [
    "Next let's see what the semantic search results would be for these questions:\n",
    "- â€œExplain the difference between supervised, unsupervised, and reinforcement learning.â€\n",
    "- â€œWhat kinds of realâ€‘world problems can machine learning solve today?â€\n",
    "- â€œHow does reinforcement learning decide which actions to take to maximize rewards?â€\n",
    "- â€œGive some examples of how machine learning is used in healthcare and fraud prevention.â€\n",
    "- â€œWhy is machine learning becoming more important as the amount of data grows?â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34b8d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Create a token provider that returns a fresh bearer token on each call\n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(),\n",
    "    \"https://cognitiveservices.azure.com/.default\",\n",
    ")\n",
    "\n",
    "aoai_client = AzureOpenAI(\n",
    "    azure_ad_token_provider=token_provider,\n",
    "    api_version=\"2024-02-01\",\n",
    ")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(search_key),\n",
    ")\n",
    "\n",
    "def embed_query(text: str, model: str = \"text-embedding-ada-002\") -> List[float]:\n",
    "    \"\"\"Create a single embedding vector for a query string using Azure OpenAI.\"\"\"\n",
    "    resp = aoai_client.embeddings.create(\n",
    "        model=model,        # Azure deployment name\n",
    "        input=[text],\n",
    "    )\n",
    "    return resp.data[0].embedding\n",
    "\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "def run_test_queries(queries: list[str], use_hybrid: bool = True, top_k: int = 3):\n",
    "    for q in queries:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Query: {q}\")\n",
    "        print(f\"Hybrid search: {use_hybrid}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        q_vector = embed_query(q)\n",
    "\n",
    "        vq = VectorizedQuery(\n",
    "            vector=q_vector,\n",
    "            fields=\"contentVector\",\n",
    "        )\n",
    "\n",
    "        if use_hybrid:\n",
    "            results = search_client.search(\n",
    "                search_text=q,\n",
    "                vector_queries=[vq],\n",
    "                top=top_k,\n",
    "            )\n",
    "        else:\n",
    "            results = search_client.search(\n",
    "                search_text=None,\n",
    "                vector_queries=[vq],\n",
    "                top=top_k,\n",
    "            )\n",
    "\n",
    "        for i, doc in enumerate(results):\n",
    "            score = doc.get(\"@search.score\", None)  # float relevance score\n",
    "            print(f\"[{i}] id={doc['id']}  score={score:.4f}\" if score is not None else f\"[{i}] id={doc['id']}\")\n",
    "            print(doc[\"content\"])\n",
    "            print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf263dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Query: Explain the difference between supervised, unsupervised, and reinforcement learning.\n",
      "Hybrid search: True\n",
      "--------------------------------------------------------------------------------\n",
      "[0] id=1  score=0.0331\n",
      "## Types of Machine Learning  \n",
      "### Supervised Learning  \n",
      "In supervised learning, the algorithm learns from labeled training data. Each training example consists of an input object and a desired output value. The algorithm analyzes the training data and produces an inferred function.\n",
      "----------------------------------------\n",
      "[1] id=2  score=0.0328\n",
      "### Unsupervised Learning  \n",
      "Unsupervised learning algorithms work with unlabeled data. The system tries to learn without a teacher, finding hidden patterns or intrinsic structures in input data.\n",
      "----------------------------------------\n",
      "[2] id=3  score=0.0325\n",
      "### Reinforcement Learning  \n",
      "Reinforcement learning is about taking suitable action to maximize reward in a particular situation. It is employed by various software and machines to find the best possible behavior or path in a specific situation.\n",
      "----------------------------------------\n",
      "================================================================================\n",
      "Query: Explain the difference between supervised, unsupervised, and reinforcement learning.\n",
      "Hybrid search: False\n",
      "--------------------------------------------------------------------------------\n",
      "[0] id=1  score=0.8660\n",
      "## Types of Machine Learning  \n",
      "### Supervised Learning  \n",
      "In supervised learning, the algorithm learns from labeled training data. Each training example consists of an input object and a desired output value. The algorithm analyzes the training data and produces an inferred function.\n",
      "----------------------------------------\n",
      "[1] id=3  score=0.8521\n",
      "### Reinforcement Learning  \n",
      "Reinforcement learning is about taking suitable action to maximize reward in a particular situation. It is employed by various software and machines to find the best possible behavior or path in a specific situation.\n",
      "----------------------------------------\n",
      "[2] id=2  score=0.8506\n",
      "### Unsupervised Learning  \n",
      "Unsupervised learning algorithms work with unlabeled data. The system tries to learn without a teacher, finding hidden patterns or intrinsic structures in input data.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "TEST_QUERIES = [\n",
    "    \"Explain the difference between supervised, unsupervised, and reinforcement learning.\",\n",
    "    #\"What kinds of real-world problems can machine learning solve today?\",\n",
    "    #\"How does reinforcement learning decide which actions to take to maximize rewards?\",\n",
    "    #\"Give some examples of how machine learning is used in healthcare and fraud prevention.\",\n",
    "    #\"Why is machine learning becoming more important as the amount of data grows?\",\n",
    "]\n",
    "\n",
    "# Hybrid on:\n",
    "run_test_queries(TEST_QUERIES, use_hybrid=True, top_k=3)\n",
    "\n",
    "# Hybrid off:\n",
    "run_test_queries(TEST_QUERIES, use_hybrid=False, top_k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ee907",
   "metadata": {},
   "source": [
    "## Step 4: Test Different Chunk Sizes and Embedding Models\n",
    "\n",
    "In this step, you get to explore the differences between using:\n",
    "- text-embedding-ada-002\n",
    "- text-embedding-3-small\n",
    "- text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "186edc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "\n",
    "class AzureSearchComparison:\n",
    "    \"\"\"\n",
    "    Compare semantic search results across different embedding model indexes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize Azure Search connection.\n",
    "        \"\"\"\n",
    "        self.endpoint = search_endpoint\n",
    "        self.credential = AzureKeyCredential(search_key)\n",
    "        \n",
    "        # Define the indexes and their embedding dimensions\n",
    "        self.indexes = {\n",
    "            'large3index': {\n",
    "                'dimensions': 3072,  # Corrected dimension for text-embedding-3-large\n",
    "                'model': 'text-embedding-3-large'\n",
    "            },\n",
    "            'small3index': {\n",
    "                'dimensions': 1536,  # Corrected dimension for text-embedding-3-small\n",
    "                'model': 'text-embedding-3-small'\n",
    "            },\n",
    "            'ada002index': {\n",
    "                'dimensions': 1536,\n",
    "                'model': 'text-embedding-ada-002'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_embedding(self, text: str, model: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Generate embedding for the query text using specified model.\n",
    "        \n",
    "        Args:\n",
    "            text: Query text to embed\n",
    "            model: OpenAI embedding model name\n",
    "            \n",
    "        Returns:\n",
    "            List of floats representing the embedding vector\n",
    "        \"\"\"\n",
    "        from openai import AzureOpenAI\n",
    "        from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "        \n",
    "        # Create a token provider that returns a fresh bearer token on each call\n",
    "        token_provider = get_bearer_token_provider(\n",
    "            DefaultAzureCredential(),\n",
    "            \"https://cognitiveservices.azure.com/.default\",\n",
    "        )\n",
    "        \n",
    "        client = AzureOpenAI(\n",
    "            azure_ad_token_provider=token_provider,\n",
    "            api_version=\"2024-02-01\",\n",
    "        )\n",
    "                \n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    def search_index(self, \n",
    "                     index_name: str, \n",
    "                     query_text: str, \n",
    "                     vector_dimensions: int,\n",
    "                     embedding_model: str,\n",
    "                     top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search a specific Azure AI Search index using vector similarity.\n",
    "        \n",
    "        Args:\n",
    "            index_name: Name of the Azure search index\n",
    "            query_text: Text query to search for\n",
    "            vector_dimensions: Dimension of the embedding vectors\n",
    "            embedding_model: OpenAI model to use for generating query embedding\n",
    "            top_k: Number of top results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of search results with scores\n",
    "        \"\"\"\n",
    "        # Create search client for this index\n",
    "        search_client = SearchClient(\n",
    "            endpoint=self.endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=self.credential\n",
    "        )\n",
    "        \n",
    "        # Generate embedding for the query\n",
    "        print(f\"  Generating embedding with {embedding_model}...\")\n",
    "        query_vector = self.get_embedding(query_text, embedding_model)\n",
    "        \n",
    "        # Create vector query\n",
    "        vector_query = VectorizedQuery(\n",
    "            vector=query_vector,\n",
    "            k=top_k,\n",
    "            fields=\"contentVector\"\n",
    "        )\n",
    "        \n",
    "        # Perform search\n",
    "        results = search_client.search(\n",
    "            search_text=None,  # Pure vector search\n",
    "            vector_queries=[vector_query],\n",
    "            select=[\"id\", \"content\"],\n",
    "            top=top_k\n",
    "        )\n",
    "        \n",
    "        # Collect results\n",
    "        search_results = []\n",
    "        for result in results:\n",
    "            search_results.append({\n",
    "                'id': result['id'],\n",
    "                'content': result['content'][:200] + \"...\" if len(result['content']) > 200 else result['content'],\n",
    "                'score': result['@search.score']\n",
    "            })\n",
    "        \n",
    "        return search_results\n",
    "    \n",
    "    def compare_search_results(self, query_text: str, top_k: int = 3):\n",
    "        \"\"\"\n",
    "        Compare search results across all three indexes.\n",
    "        \n",
    "        Args:\n",
    "            query_text: The search query\n",
    "            top_k: Number of top results to show per index\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ðŸ” SEMANTIC SEARCH COMPARISON\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nQuery: '{query_text}'\\n\")\n",
    "        print(f\"Retrieving top {top_k} results from each index...\\n\")\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        # Search each index\n",
    "        for index_name, index_info in self.indexes.items():\n",
    "            print(f\"\\nðŸ“Š Searching {index_name.upper()}\")\n",
    "            print(f\"   Model: {index_info['model']}\")\n",
    "            print(f\"   Dimensions: {index_info['dimensions']}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            try:\n",
    "                results = self.search_index(\n",
    "                    index_name=index_name,\n",
    "                    query_text=query_text,\n",
    "                    vector_dimensions=index_info['dimensions'],\n",
    "                    embedding_model=index_info['model'],\n",
    "                    top_k=top_k\n",
    "                )\n",
    "                \n",
    "                all_results[index_name] = results\n",
    "                \n",
    "                # Display results for this index\n",
    "                for i, result in enumerate(results, 1):\n",
    "                    print(f\"\\n   Result {i} (Score: {result['score']:.4f}):\")\n",
    "                    print(f\"   ID: {result['id']}\")\n",
    "                    print(f\"   Content: {result['content']}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error searching {index_name}: {str(e)}\")\n",
    "                all_results[index_name] = []\n",
    "        \n",
    "        # Compare and analyze differences\n",
    "        self.analyze_differences(all_results, query_text)\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def analyze_differences(self, all_results: Dict[str, List[Dict]], query_text: str):\n",
    "        \"\"\"\n",
    "        Analyze and highlight differences between search results.\n",
    "        \n",
    "        Args:\n",
    "            all_results: Dictionary of results from each index\n",
    "            query_text: Original query text\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ðŸ“ˆ ANALYSIS: Differences Between Embedding Models\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Check if all indexes returned results\n",
    "        indexes_with_results = [idx for idx, results in all_results.items() if results]\n",
    "        \n",
    "        if len(indexes_with_results) < 2:\n",
    "            print(\"\\nâš ï¸  Not enough results to compare. Check your indexes and API keys.\")\n",
    "            return\n",
    "        \n",
    "        # Compare top results\n",
    "        print(\"\\nðŸŽ¯ Top Result Comparison:\")\n",
    "        print(\"-\" * 40)\n",
    "        for index_name, results in all_results.items():\n",
    "            if results:\n",
    "                top_result = results[0]\n",
    "                print(f\"\\n{index_name}:\")\n",
    "                print(f\"  Top match ID: {top_result['id']}\")\n",
    "                print(f\"  Score: {top_result['score']:.4f}\")\n",
    "        \n",
    "        # Check for agreement on top result\n",
    "        top_ids = [results[0]['id'] for results in all_results.values() if results]\n",
    "        if len(set(top_ids)) == 1:\n",
    "            print(\"\\nâœ… All models agree on the top result!\")\n",
    "        else:\n",
    "            print(\"\\nðŸ”„ Models returned different top results\")\n",
    "            \n",
    "        # Calculate overlap in results\n",
    "        print(\"\\nðŸ“Š Result Overlap Analysis:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Get all unique IDs per index\n",
    "        for i, (idx1, results1) in enumerate(all_results.items()):\n",
    "            if not results1:\n",
    "                continue\n",
    "            ids1 = set(r['id'] for r in results1)\n",
    "            \n",
    "            for idx2, results2 in list(all_results.items())[i+1:]:\n",
    "                if not results2:\n",
    "                    continue\n",
    "                ids2 = set(r['id'] for r in results2)\n",
    "                \n",
    "                overlap = ids1.intersection(ids2)\n",
    "                overlap_pct = (len(overlap) / max(len(ids1), len(ids2))) * 100\n",
    "                \n",
    "                print(f\"\\n{idx1} vs {idx2}:\")\n",
    "                print(f\"  Overlapping results: {len(overlap)}/{max(len(ids1), len(ids2))}\")\n",
    "                print(f\"  Similarity: {overlap_pct:.1f}%\")\n",
    "                \n",
    "                if overlap:\n",
    "                    print(f\"  Common IDs: {', '.join(sorted(overlap))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d787d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting semantic search comparison across embedding models...\n",
      "\n",
      "================================================================================\n",
      "ðŸ” SEMANTIC SEARCH COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Query: 'Explain the difference between supervised, unsupervised, and reinforcement learning.'\n",
      "\n",
      "Retrieving top 3 results from each index...\n",
      "\n",
      "\n",
      "ðŸ“Š Searching LARGE3INDEX\n",
      "   Model: text-embedding-3-large\n",
      "   Dimensions: 3072\n",
      "------------------------------------------------------------\n",
      "  Generating embedding with text-embedding-3-large...\n",
      "   âŒ Error searching large3index: () The index 'large3index' for service 'srcjan2026afworkshop' was not found.\n",
      "Code: \n",
      "Message: The index 'large3index' for service 'srcjan2026afworkshop' was not found.\n",
      "\n",
      "ðŸ“Š Searching SMALL3INDEX\n",
      "   Model: text-embedding-3-small\n",
      "   Dimensions: 1536\n",
      "------------------------------------------------------------\n",
      "  Generating embedding with text-embedding-3-small...\n",
      "   âŒ Error searching small3index: () The index 'small3index' for service 'srcjan2026afworkshop' was not found.\n",
      "Code: \n",
      "Message: The index 'small3index' for service 'srcjan2026afworkshop' was not found.\n",
      "\n",
      "ðŸ“Š Searching ADA002INDEX\n",
      "   Model: text-embedding-ada-002\n",
      "   Dimensions: 1536\n",
      "------------------------------------------------------------\n",
      "  Generating embedding with text-embedding-ada-002...\n",
      "   âŒ Error searching ada002index: () The index 'ada002index' for service 'srcjan2026afworkshop' was not found.\n",
      "Code: \n",
      "Message: The index 'ada002index' for service 'srcjan2026afworkshop' was not found.\n",
      "\n",
      "================================================================================\n",
      "ðŸ“ˆ ANALYSIS: Differences Between Embedding Models\n",
      "================================================================================\n",
      "\n",
      "âš ï¸  Not enough results to compare. Check your indexes and API keys.\n",
      "\n",
      "================================================================================\n",
      "ðŸ’¡ KEY INSIGHTS FOR STUDENTS:\n",
      "================================================================================\n",
      "\n",
      "1. DIMENSIONALITY: \n",
      "    - text-embedding-3-large (3072 dims) captures more nuanced relationships\n",
      "    - text-embedding-ada-002 and text-embedding-3-small (1536 dims) are more efficient\n",
      "\n",
      "2. PERFORMANCE VS COST:\n",
      "    - Larger models may provide better semantic understanding\n",
      "    - Smaller models are faster and cheaper to run at scale\n",
      "\n",
      "3. USE CASE CONSIDERATIONS:\n",
      "    - For high-precision tasks: Consider larger embedding models\n",
      "    - For high-throughput applications: Smaller models may be sufficient\n",
      "    - Always test with your specific data and queries\n",
      "\n",
      "4. WHAT TO LOOK FOR:\n",
      "    - Do all models find the same top result?\n",
      "    - How much overlap is there in the top 3 results?\n",
      "    - Are the relevance scores significantly different?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the comparison tool\n",
    "searcher = AzureSearchComparison()\n",
    "\n",
    "# The search query\n",
    "query = \"Explain the difference between supervised, unsupervised, and reinforcement learning.\"\n",
    "\n",
    "# Run the comparison\n",
    "print(\"\\nðŸš€ Starting semantic search comparison across embedding models...\")\n",
    "results = searcher.compare_search_results(query, top_k=3)\n",
    "\n",
    "# Additional insights\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ’¡ KEY INSIGHTS FOR STUDENTS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "1. DIMENSIONALITY: \n",
    "    - text-embedding-3-large (3072 dims) captures more nuanced relationships\n",
    "    - text-embedding-ada-002 and text-embedding-3-small (1536 dims) are more efficient\n",
    "\n",
    "2. PERFORMANCE VS COST:\n",
    "    - Larger models may provide better semantic understanding\n",
    "    - Smaller models are faster and cheaper to run at scale\n",
    "\n",
    "3. USE CASE CONSIDERATIONS:\n",
    "    - For high-precision tasks: Consider larger embedding models\n",
    "    - For high-throughput applications: Smaller models may be sufficient\n",
    "    - Always test with your specific data and queries\n",
    "\n",
    "4. WHAT TO LOOK FOR:\n",
    "    - Do all models find the same top result?\n",
    "    - How much overlap is there in the top 3 results?\n",
    "    - Are the relevance scores significantly different?\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
