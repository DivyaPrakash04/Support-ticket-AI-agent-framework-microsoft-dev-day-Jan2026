{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77398ed",
   "metadata": {},
   "source": [
    "# Model Context Protocol (MCP) Concepts\n",
    "\n",
    "The **Model Context Protocol (MCP)** is an open protocol that standardizes how applications provide context to Large Language Models (LLMs). Think of it as a universal adapter that allows AI assistants to connect to various data sources and tools.\n",
    "\n",
    "> ðŸ“ **Hands-on Exercises**: After reviewing these concepts, complete the exercises in [EXERCISES.md](EXERCISES.md).\n",
    "\n",
    "## Key Benefits\n",
    "- **Standardization**: One protocol to connect to many tools\n",
    "- **Security**: Controlled access to resources\n",
    "- **Flexibility**: Works with any LLM provider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd969f20",
   "metadata": {},
   "source": [
    "## MCP Architecture\n",
    "\n",
    "```\n",
    "+-----------------+     +-----------------+     +-----------------+\n",
    "|   AI Agent      |---->|   MCP Client    |---->|   MCP Server    |\n",
    "|   (Host)        |     |                 |     |   (Tools)       |\n",
    "+-----------------+     +-----------------+     +-----------------+\n",
    "```\n",
    "\n",
    "### Components\n",
    "1. **Host**: The AI application (e.g., Claude, ChatGPT integration, or custom agent)\n",
    "2. **Client**: Connects to MCP servers on behalf of the host\n",
    "3. **Server**: Exposes tools, resources, and prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38883d6",
   "metadata": {},
   "source": [
    "## Transport Types\n",
    "\n",
    "MCP supports two primary transport mechanisms:\n",
    "\n",
    "### 1. STDIO (Standard Input/Output)\n",
    "- Used for **local** MCP servers\n",
    "- Server runs as a subprocess\n",
    "- Communication via stdin/stdout\n",
    "- Best for: Local tools, CLI applications\n",
    "\n",
    "### 2. HTTP/SSE (Server-Sent Events)\n",
    "- Used for **remote** MCP servers\n",
    "- Server runs as a web service\n",
    "- Communication via HTTP requests and SSE\n",
    "- Best for: Cloud services, shared tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9f8bb",
   "metadata": {},
   "source": [
    "## Workshop Projects Overview\n",
    "\n",
    "| Project | Type | Transport | Description |\n",
    "|---------|------|-----------|-------------|\n",
    "| mcp_local_server | Local | STDIO | Python MCP server with Config and Ticket tools |\n",
    "| mcp_remote_server | REST API | HTTP | Backend REST API for tickets |\n",
    "| mcp_bridge | Remote | HTTP/SSE | MCP server that calls REST API |\n",
    "| mcp_agent_client | Client | Both | AI agent that consumes MCP servers |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ce7511",
   "metadata": {},
   "source": [
    "## Setup: Install Required Packages\n",
    "\n",
    "Run the following cell to install the required packages for MCP and Azure OpenAI integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964fd9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install required packages\n",
    "# Run this cell first to ensure all dependencies are installed\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"mcp\",\n",
    "    \"httpx\",\n",
    "    \"httpx-sse\", \n",
    "    \"openai\",\n",
    "    \"azure-identity\",\n",
    "    \"python-dotenv\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    \n",
    "print(\"âœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38167e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: Load environment variables\n",
    "# Set up your Azure OpenAI connection\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def find_config_path(start_path: str) -> str:\n",
    "    \"\"\"Find the 'python' folder by traversing up from start_path.\"\"\"\n",
    "    current_dir = Path(start_path)\n",
    "    \n",
    "    while current_dir is not None:\n",
    "        if current_dir.name.lower() == \"python\":\n",
    "            return str(current_dir)\n",
    "        if current_dir.parent == current_dir:\n",
    "            break\n",
    "        current_dir = current_dir.parent\n",
    "    \n",
    "    # Fallback to start path if python folder not found\n",
    "    return start_path\n",
    "\n",
    "\n",
    "def load_env_file(env_path: str) -> dict:\n",
    "    \"\"\"Load environment variables from .env file (JSON format).\"\"\"\n",
    "    env_file = Path(env_path) / \".env\"\n",
    "    \n",
    "    if not env_file.exists():\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        with open(env_file, 'r') as f:\n",
    "            content = f.read()\n",
    "            env_vars = json.loads(content)\n",
    "            \n",
    "            # Set environment variables\n",
    "            for key, value in env_vars.items():\n",
    "                os.environ[key] = str(value)\n",
    "            \n",
    "            return env_vars\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: try loading as standard dotenv format\n",
    "        load_dotenv(env_file, override=True)\n",
    "        return {}\n",
    "    except IOError as e:\n",
    "        print(f\"Warning: Failed to load .env file: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Load environment variables from .env file in the python root folder\n",
    "config_path = find_config_path(os.getcwd())\n",
    "env_vars = load_env_file(config_path)\n",
    "if env_vars:\n",
    "    print(f\"âœ… Loaded {len(env_vars)} environment variables from: {config_path}/.env\")\n",
    "else:\n",
    "    # Fallback: try loading from current directory\n",
    "    load_dotenv()\n",
    "    print(\"âš ï¸ Loaded .env from current directory (fallback)\")\n",
    "\n",
    "# Check required configuration - support both naming conventions\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\") or os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\") or os.getenv(\"AZURE_AI_MODEL_DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
    "\n",
    "if endpoint:\n",
    "    print(f\"âœ… Azure OpenAI Endpoint: {endpoint}\")\n",
    "    print(f\"âœ… Deployment Name: {deployment}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Azure endpoint not set\")\n",
    "    print(\"Please set one of the following environment variables:\")\n",
    "    print(\"  - AZURE_AI_PROJECT_ENDPOINT or AZURE_OPENAI_ENDPOINT\")\n",
    "    print(\"And optionally:\")\n",
    "    print(\"  - AZURE_AI_MODEL_DEPLOYMENT_NAME or AZURE_OPENAI_DEPLOYMENT_NAME (default: gpt-4o-mini)\")\n",
    "    print(\"  - AZURE_OPENAI_API_KEY (for API key auth)\")\n",
    "\n",
    "# Check authentication method\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "tenant_id = os.getenv(\"AZURE_TENANT_ID\")\n",
    "client_id = os.getenv(\"AZURE_CLIENT_ID\")\n",
    "client_secret = os.getenv(\"AZURE_CLIENT_SECRET\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"âœ… Authentication: API Key\")\n",
    "elif tenant_id and client_id and client_secret:\n",
    "    print(\"âœ… Authentication: Service Principal\")\n",
    "else:\n",
    "    print(\"âœ… Authentication: Azure CLI / DefaultAzureCredential\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Azure OpenAI Client\n",
    "# This demonstrates how to set up the client with different auth methods\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, AzureCliCredential, ClientSecretCredential, get_bearer_token_provider\n",
    "\n",
    "# Get configuration - AZURE_OPENAI takes priority over AZURE_AI_PROJECT\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
    "\n",
    "# Only fall back to Foundry endpoint if Azure OpenAI endpoint is not set\n",
    "if not endpoint:\n",
    "    endpoint = os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "    deployment = os.getenv(\"AZURE_AI_MODEL_DEPLOYMENT_NAME\", deployment)\n",
    "    print(\"âš ï¸ Using Azure AI Foundry endpoint (AZURE_AI_PROJECT_ENDPOINT)\")\n",
    "\n",
    "if not endpoint:\n",
    "    raise ValueError(\"Azure endpoint not set. Set AZURE_OPENAI_ENDPOINT environment variable.\")\n",
    "\n",
    "api_version = \"2024-02-15-preview\"\n",
    "\n",
    "# Use API Key if provided (standard Azure OpenAI)\n",
    "if api_key:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_key=api_key,\n",
    "        api_version=api_version\n",
    "    )\n",
    "    print(f\"âœ… Created AzureOpenAI client with API Key auth\")\n",
    "    print(f\"   Endpoint: {endpoint}\")\n",
    "    print(f\"   Deployment: {deployment}\")\n",
    "else:\n",
    "    # Token-based auth (for Foundry endpoints or when no API key)\n",
    "    print(\"âš ï¸ No API key found, using token-based authentication\")\n",
    "    try:\n",
    "        credential = AzureCliCredential()\n",
    "        token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=endpoint,\n",
    "            azure_ad_token_provider=token_provider,\n",
    "            api_version=api_version\n",
    "        )\n",
    "        print(\"âœ… Created AzureOpenAI client with Azure CLI auth\")\n",
    "    except Exception as e:\n",
    "        credential = DefaultAzureCredential()\n",
    "        token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=endpoint,\n",
    "            azure_ad_token_provider=token_provider,\n",
    "            api_version=api_version\n",
    "        )\n",
    "        print(\"âœ… Created AzureOpenAI client with DefaultAzureCredential\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check your configuration values\n",
    "import os\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\") or os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\") or os.getenv(\"AZURE_AI_MODEL_DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
    "\n",
    "print(\"=== Configuration Check ===\")\n",
    "print(f\"Endpoint: {endpoint}\")\n",
    "print(f\"API Key: {'*' * 10 + api_key[-4:] if api_key and len(api_key) > 4 else 'NOT SET or too short'}\")\n",
    "print(f\"Deployment: {deployment}\")\n",
    "\n",
    "# Validate endpoint format\n",
    "if endpoint:\n",
    "    if not endpoint.startswith(\"https://\"):\n",
    "        print(\"âš ï¸ Endpoint should start with 'https://'\")\n",
    "    if not \"openai.azure.com\" in endpoint:\n",
    "        print(\"âš ï¸ Endpoint should contain 'openai.azure.com'\")\n",
    "    if endpoint.endswith(\"/\"):\n",
    "        print(\"âš ï¸ Endpoint should NOT end with '/' - this can cause auth errors\")\n",
    "else:\n",
    "    print(\"âŒ Endpoint is not set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Azure OpenAI connection\n",
    "# Send a simple completion request to verify everything is working\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Say 'Hello from MCP Lab!' in exactly 5 words.\"}\n",
    "    ],\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "print(\"Response from Azure OpenAI:\")\n",
    "print(f\"  {response.choices[0].message.content}\")\n",
    "print()\n",
    "print(f\"âœ… Azure OpenAI connection successful!\")\n",
    "print(f\"  Model: {response.model}\")\n",
    "print(f\"  Tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP Server Example - Define tools\n",
    "# This shows how to create MCP tools\n",
    "\n",
    "from mcp.server import Server\n",
    "from mcp.types import Tool, TextContent\n",
    "\n",
    "# Create an MCP server instance\n",
    "server = Server(\"example-server\")\n",
    "\n",
    "# Define a simple tool\n",
    "@server.list_tools()\n",
    "async def list_tools() -> list[Tool]:\n",
    "    \"\"\"List available tools.\"\"\"\n",
    "    return [\n",
    "        Tool(\n",
    "            name=\"GetConfig\",\n",
    "            description=\"Gets a configuration value by key\",\n",
    "            inputSchema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"key\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The configuration key\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"key\"]\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    "\n",
    "print(\"âœ… MCP Server defined with tools\")\n",
    "print(\"Available tools:\")\n",
    "tools = await list_tools()\n",
    "for tool in tools:\n",
    "    print(f\"  - {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204f278",
   "metadata": {},
   "source": [
    "#  MCP Concepts - Python\n",
    "\n",
    "This notebook explains the key concepts of the **Model Context Protocol (MCP)** and how to use it with Python.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [What is MCP?](#what-is-mcp)\n",
    "2. [MCP Architecture](#mcp-architecture)\n",
    "3. [Transport Types](#transport-types)\n",
    "4. [Workshop Projects Overview](#workshop-projects-overview)\n",
    "5. [Creating an MCP Server](#creating-an-mcp-server)\n",
    "6. [Creating an MCP Client](#creating-an-mcp-client)\n",
    "7. [AI Agent Integration](#ai-agent-integration)\n",
    "8. [HTTP/SSE Transport](#httpsse-transport)\n",
    "9. [Best Practices](#best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3f666",
   "metadata": {},
   "source": [
    "## What is MCP?\n",
    "\n",
    "The **Model Context Protocol (MCP)** is an open protocol that standardizes how applications provide context to Large Language Models (LLMs). Think of it as a universal adapter that allows AI assistants to connect to various data sources and tools.\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **Standardization** | One protocol to connect to many tools |\n",
    "| **Security** | Controlled access to resources |\n",
    "| **Flexibility** | Works with any LLM provider |\n",
    "| **Interoperability** | Tools written in any language can be consumed |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arch-section",
   "metadata": {},
   "source": [
    "## MCP Architecture\n",
    "\n",
    "`\n",
    "          \n",
    "   AI Agent         MCP Client       MCP Server    \n",
    "   (Host)                                      (Tools)       \n",
    "          \n",
    "`\n",
    "\n",
    "### Components\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Host** | The AI application (e.g., Claude, ChatGPT integration) |\n",
    "| **Client** | Connects to MCP servers on behalf of the host |\n",
    "| **Server** | Exposes tools, resources, and prompts |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transport-section",
   "metadata": {},
   "source": [
    "## Transport Types\n",
    "\n",
    "MCP supports two primary transport mechanisms:\n",
    "\n",
    "### 1. STDIO (Standard Input/Output)\n",
    "- Used for **local** MCP servers\n",
    "- Server runs as a subprocess\n",
    "- Communication via stdin/stdout\n",
    "- Best for: Local tools, CLI applications\n",
    "\n",
    "### 2. HTTP/SSE (Server-Sent Events)\n",
    "- Used for **remote** MCP servers\n",
    "- Server runs as a web service\n",
    "- Communication via HTTP requests and SSE\n",
    "- Best for: Cloud services, shared tools\n",
    "\n",
    "### When to Use Which?\n",
    "\n",
    "| Scenario | Transport | Reason |\n",
    "|----------|-----------|--------|\n",
    "| Local file access | STDIO | Direct filesystem access |\n",
    "| Low latency critical | STDIO | No network overhead |\n",
    "| Remote API access | HTTP/SSE | Network communication |\n",
    "| Multi-user scenarios | HTTP/SSE | Shared server instance |\n",
    "| Centralized monitoring | HTTP/SSE | Server-side logging |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "projects-section",
   "metadata": {},
   "source": [
    "## Workshop Projects Overview\n",
    "\n",
    "| Project | Type | Transport | Description |\n",
    "|---------|------|-----------|-------------|\n",
    "| mcp_local_server | Local | STDIO | Python MCP server with Config and Ticket tools |\n",
    "| mcp_remote_server | REST API | HTTP | FastAPI backend REST API for tickets |\n",
    "| mcp_bridge | Remote | HTTP/SSE | MCP server that calls REST API |\n",
    "| mcp_agent_client | Client | Both | AI agent that consumes MCP servers |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "server-section",
   "metadata": {},
   "source": [
    "## Creating an MCP Server\n",
    "\n",
    "### Basic Server Structure\n",
    "\n",
    "`python\n",
    "from mcp.server import Server\n",
    "from mcp.server.stdio import stdio_server\n",
    "import asyncio\n",
    "\n",
    "# Create server instance\n",
    "server = Server(\"mcp-local-server\")\n",
    "`\n",
    "\n",
    "### Defining Tools with Decorators\n",
    "\n",
    "`python\n",
    "@server.tool()\n",
    "async def get_config() -> str:\n",
    "    \"\"\"Gets the current configuration.\"\"\"\n",
    "    return \"Current config value\"\n",
    "\n",
    "@server.tool()\n",
    "async def update_config(value: str) -> str:\n",
    "    \"\"\"Updates the configuration.\n",
    "    \n",
    "    Args:\n",
    "        value: The new configuration value\n",
    "    \"\"\"\n",
    "    return f\"Updated to: {value}\"\n",
    "`\n",
    "\n",
    "### Tool with Complex Parameters\n",
    "\n",
    "`python\n",
    "from typing import Optional\n",
    "\n",
    "@server.tool()\n",
    "async def create_ticket(\n",
    "    subject: str,\n",
    "    description: str,\n",
    "    customer_name: str,\n",
    "    priority: str = \"MEDIUM\"\n",
    ") -> str:\n",
    "    \"\"\"Creates a new support ticket.\n",
    "    \n",
    "    Args:\n",
    "        subject: Brief summary of the issue\n",
    "        description: Detailed description of the problem\n",
    "        customer_name: Name of the customer\n",
    "        priority: Priority level (LOW, MEDIUM, HIGH)\n",
    "    \"\"\"\n",
    "    ticket_id = str(uuid.uuid4())[:8]\n",
    "    # Store ticket logic here\n",
    "    return f\"Ticket {ticket_id} created successfully\"\n",
    "`\n",
    "\n",
    "### Running the Server (STDIO)\n",
    "\n",
    "`python\n",
    "async def main():\n",
    "    async with stdio_server() as (read_stream, write_stream):\n",
    "        await server.run(\n",
    "            read_stream,\n",
    "            write_stream,\n",
    "            server.create_initialization_options()\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "client-section",
   "metadata": {},
   "source": [
    "## Creating an MCP Client\n",
    "\n",
    "### STDIO Client Connection\n",
    "\n",
    "`python\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "# Define server parameters\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"-m\", \"mcp_local_server.main\"]\n",
    ")\n",
    "\n",
    "# Connect and use\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "        \n",
    "        # List available tools\n",
    "        tools = await session.list_tools()\n",
    "        for tool in tools.tools:\n",
    "            print(f\"Tool: {tool.name} - {tool.description}\")\n",
    "        \n",
    "        # Call a tool\n",
    "        result = await session.call_tool(\"get_config\", {})\n",
    "        print(f\"Result: {result.content[0].text}\")\n",
    "`\n",
    "\n",
    "### SSE Client Connection (Remote)\n",
    "\n",
    "`python\n",
    "from mcp.client.sse import sse_client\n",
    "\n",
    "async with sse_client(\"http://localhost:5070/sse\") as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "        \n",
    "        # Use remote tools\n",
    "        result = await session.call_tool(\"get_all_tickets\", {})\n",
    "        print(result.content[0].text)\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ai-agent-section",
   "metadata": {},
   "source": [
    "## AI Agent Integration\n",
    "\n",
    "The real power of MCP comes when integrating it with AI agents.\n",
    "\n",
    "### Converting MCP Tools to OpenAI Format\n",
    "\n",
    "`python\n",
    "def mcp_tools_to_openai_format(mcp_tools):\n",
    "    \"\"\"Convert MCP tools to OpenAI function calling format.\"\"\"\n",
    "    openai_tools = []\n",
    "    for tool in mcp_tools.tools:\n",
    "        openai_tool = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description or \"\",\n",
    "                \"parameters\": tool.inputSchema if tool.inputSchema else {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {},\n",
    "                    \"required\": []\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        openai_tools.append(openai_tool)\n",
    "    return openai_tools\n",
    "`\n",
    "\n",
    "### Creating the Azure OpenAI Client\n",
    "\n",
    "`python\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import AzureCliCredential, DefaultAzureCredential\n",
    "\n",
    "def create_chat_client() -> AzureOpenAI:\n",
    "    \"\"\"Create Azure OpenAI client with multiple auth options.\"\"\"\n",
    "    endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "    \n",
    "    if api_key:\n",
    "        return AzureOpenAI(\n",
    "            azure_endpoint=endpoint,\n",
    "            api_key=api_key,\n",
    "            api_version=\"2024-02-15-preview\"\n",
    "        )\n",
    "    else:\n",
    "        credential = DefaultAzureCredential()\n",
    "        token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "        return AzureOpenAI(\n",
    "            azure_endpoint=endpoint,\n",
    "            api_key=token.token,\n",
    "            api_version=\"2024-02-15-preview\"\n",
    "        )\n",
    "`\n",
    "\n",
    "### AI Agent with Tool Calling\n",
    "\n",
    "`python\n",
    "import json\n",
    "\n",
    "async def run_agent_with_tools(client, session, user_message: str):\n",
    "    \"\"\"Run an AI agent that can use MCP tools.\"\"\"\n",
    "    \n",
    "    # Get MCP tools and convert to OpenAI format\n",
    "    mcp_tools = await session.list_tools()\n",
    "    openai_tools = mcp_tools_to_openai_format(mcp_tools)\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant with access to support ticket tools.\"},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # Call the AI model\n",
    "    response = client.chat.completions.create(\n",
    "        model=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4o-mini\"),\n",
    "        messages=messages,\n",
    "        tools=openai_tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    \n",
    "    assistant_message = response.choices[0].message\n",
    "    \n",
    "    # Handle tool calls\n",
    "    if assistant_message.tool_calls:\n",
    "        for tool_call in assistant_message.tool_calls:\n",
    "            tool_name = tool_call.function.name\n",
    "            tool_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            # Execute MCP tool\n",
    "            result = await session.call_tool(tool_name, tool_args)\n",
    "            print(f\"Tool {tool_name} returned: {result.content[0].text}\")\n",
    "    \n",
    "    return assistant_message.content\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sse-section",
   "metadata": {},
   "source": [
    "## HTTP/SSE Transport\n",
    "\n",
    "### Server-Side Setup (FastAPI + Starlette)\n",
    "\n",
    "`python\n",
    "from mcp.server import Server\n",
    "from mcp.server.sse import SseServerTransport\n",
    "from starlette.applications import Starlette\n",
    "from starlette.routing import Route\n",
    "import uvicorn\n",
    "\n",
    "# Create MCP server and SSE transport\n",
    "server = Server(\"mcp-bridge-server\")\n",
    "sse = SseServerTransport(\"/messages\")\n",
    "\n",
    "async def handle_sse(request):\n",
    "    async with sse.connect_sse(\n",
    "        request.scope,\n",
    "        request.receive,\n",
    "        request._send\n",
    "    ) as streams:\n",
    "        await server.run(\n",
    "            streams[0],\n",
    "            streams[1],\n",
    "            server.create_initialization_options()\n",
    "        )\n",
    "\n",
    "# Define routes\n",
    "app = Starlette(\n",
    "    debug=True,\n",
    "    routes=[\n",
    "        Route(\"/sse\", endpoint=handle_sse),\n",
    "        Route(\"/messages\", endpoint=sse.handle_post_message, methods=[\"POST\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=5070)\n",
    "`\n",
    "\n",
    "### MCP Bridge Pattern\n",
    "\n",
    "The MCP Bridge pattern wraps a REST API with MCP tools:\n",
    "\n",
    "`python\n",
    "import httpx\n",
    "\n",
    "REST_API_URL = \"http://localhost:5060\"\n",
    "\n",
    "@server.tool()\n",
    "async def get_all_tickets() -> str:\n",
    "    \"\"\"Gets all support tickets from the remote API.\"\"\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(f\"{REST_API_URL}/tickets\")\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "\n",
    "@server.tool()\n",
    "async def create_ticket(\n",
    "    subject: str,\n",
    "    description: str,\n",
    "    customer_name: str\n",
    ") -> str:\n",
    "    \"\"\"Creates a new ticket via the remote API.\"\"\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.post(\n",
    "            f\"{REST_API_URL}/tickets\",\n",
    "            json={\n",
    "                \"subject\": subject,\n",
    "                \"description\": description,\n",
    "                \"customer_name\": customer_name\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Tool Design\n",
    "\n",
    "`python\n",
    "#  Good: Clear docstrings, typed parameters\n",
    "@server.tool()\n",
    "async def get_ticket(ticket_id: str) -> str:\n",
    "    \"\"\"Retrieves a support ticket by its ID.\n",
    "    \n",
    "    Args:\n",
    "        ticket_id: The unique identifier of the ticket (e.g., 'TKT-001')\n",
    "    \n",
    "    Returns:\n",
    "        JSON string containing ticket details\n",
    "    \"\"\"\n",
    "    # Implementation\n",
    "\n",
    "#  Bad: No documentation, unclear parameters\n",
    "@server.tool()\n",
    "async def get(id):\n",
    "    # Implementation\n",
    "`\n",
    "\n",
    "### Error Handling\n",
    "\n",
    "`python\n",
    "@server.tool()\n",
    "async def get_ticket(ticket_id: str) -> str:\n",
    "    \"\"\"Retrieves a support ticket.\"\"\"\n",
    "    try:\n",
    "        ticket = await fetch_ticket(ticket_id)\n",
    "        if not ticket:\n",
    "            return json.dumps({\"error\": f\"Ticket {ticket_id} not found\"})\n",
    "        return json.dumps(ticket)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "`\n",
    "\n",
    "### Authentication Options\n",
    "\n",
    "| Method | Environment Variables | Use Case |\n",
    "|--------|----------------------|----------|\n",
    "| API Key | AZURE_OPENAI_API_KEY | Development |\n",
    "| Azure CLI | None (uses az login) | Local development |\n",
    "| Service Principal | AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET | CI/CD |\n",
    "| Managed Identity | None (automatic) | Production Azure |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-section",
   "metadata": {},
   "source": [
    "## Running the Workshop\n",
    "\n",
    "### Step 1: Install Dependencies\n",
    "`bash\n",
    "cd python/lab3-mcp\n",
    "pip install -r requirements.txt\n",
    "`\n",
    "\n",
    "### Step 2: Configure Environment\n",
    "`bash\n",
    "# Set Azure OpenAI configuration\n",
    "export AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com\"\n",
    "export AZURE_OPENAI_DEPLOYMENT_NAME=\"gpt-4o-mini\"\n",
    "\n",
    "# Option A: API Key\n",
    "export AZURE_OPENAI_API_KEY=\"your-api-key\"\n",
    "\n",
    "# Option B: Azure CLI (login first)\n",
    "az login\n",
    "`\n",
    "\n",
    "### Step 3: Start the REST API\n",
    "`bash\n",
    "python -m mcp_remote_server.main  # Starts on port 5060\n",
    "`\n",
    "\n",
    "### Step 4: Start the MCP Bridge (optional)\n",
    "`bash\n",
    "python -m mcp_bridge.main  # Starts on port 5070\n",
    "`\n",
    "\n",
    "### Step 5: Run the Agent Client\n",
    "`bash\n",
    "python -m mcp_agent_client.main\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **MCP** | Protocol for AI-to-tool communication |\n",
    "| **STDIO Transport** | Local subprocess communication |\n",
    "| **HTTP/SSE Transport** | Remote web service communication |\n",
    "| **Tools** | Functions exposed by MCP servers |\n",
    "| **@server.tool()** | Decorator to mark a function as an MCP tool |\n",
    "| **ClientSession** | Client for connecting to MCP servers |\n",
    "| **Bridge Pattern** | Wrap REST APIs with MCP interface |\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "`python\n",
    "# Create server\n",
    "server = Server(\"my-server\")\n",
    "\n",
    "# Define tool\n",
    "@server.tool()\n",
    "async def my_tool(param: str) -> str:\n",
    "    \"\"\"Tool description.\"\"\"\n",
    "    return \"result\"\n",
    "\n",
    "# STDIO client\n",
    "async with stdio_client(params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "        result = await session.call_tool(\"my_tool\", {\"param\": \"value\"})\n",
    "\n",
    "# SSE client\n",
    "async with sse_client(\"http://localhost:5070/sse\") as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
