{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77398ed",
   "metadata": {},
   "source": [
    "# Model Context Protocol (MCP) Concepts\n",
    "\n",
    "The **Model Context Protocol (MCP)** is an open protocol that standardizes how applications provide context to Large Language Models (LLMs). Think of it as a universal adapter that allows AI assistants to connect to various data sources and tools.\n",
    "\n",
    "> ðŸ“ **Hands-on Exercises**: After reviewing these concepts, complete the exercises in [EXERCISES.md](EXERCISES.md).\n",
    "\n",
    "## Key Benefits\n",
    "- **Standardization**: One protocol to connect to many tools\n",
    "- **Security**: Controlled access to resources\n",
    "- **Flexibility**: Works with any LLM provider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd969f20",
   "metadata": {},
   "source": [
    "## MCP Architecture\n",
    "\n",
    "```\n",
    "+-----------------+     +-----------------+     +-----------------+\n",
    "|   AI Agent      |---->|   MCP Client    |---->|   MCP Server    |\n",
    "|   (Host)        |     |                 |     |   (Tools)       |\n",
    "+-----------------+     +-----------------+     +-----------------+\n",
    "```\n",
    "\n",
    "### Components\n",
    "1. **Host**: The AI application (e.g., Claude, ChatGPT integration, or custom agent)\n",
    "2. **Client**: Connects to MCP servers on behalf of the host\n",
    "3. **Server**: Exposes tools, resources, and prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38883d6",
   "metadata": {},
   "source": [
    "## Transport Types\n",
    "\n",
    "MCP supports two primary transport mechanisms:\n",
    "\n",
    "### 1. STDIO (Standard Input/Output)\n",
    "- Used for **local** MCP servers\n",
    "- Server runs as a subprocess\n",
    "- Communication via stdin/stdout\n",
    "- Best for: Local tools, CLI applications\n",
    "\n",
    "### 2. HTTP/SSE (Server-Sent Events)\n",
    "- Used for **remote** MCP servers\n",
    "- Server runs as a web service\n",
    "- Communication via HTTP requests and SSE\n",
    "- Best for: Cloud services, shared tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9f8bb",
   "metadata": {},
   "source": [
    "## Workshop Projects Overview\n",
    "\n",
    "| Project | Type | Transport | Description |\n",
    "|---------|------|-----------|-------------|\n",
    "| mcp_local_server | Local | STDIO | Python MCP server with Config and Ticket tools |\n",
    "| mcp_remote_server | REST API | HTTP | Backend REST API for tickets |\n",
    "| mcp_bridge | Remote | HTTP/SSE | MCP server that calls REST API |\n",
    "| mcp_agent_client | Client | Both | AI agent that consumes MCP servers |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ce7511",
   "metadata": {},
   "source": [
    "## Setup: Install Required Packages\n",
    "\n",
    "Run the following cell to install the required packages for MCP and Azure OpenAI integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964fd9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Install required packages\n",
    "# Run this cell first to ensure all dependencies are installed\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"mcp\",\n",
    "    \"httpx\",\n",
    "    \"httpx-sse\", \n",
    "    \"openai\",\n",
    "    \"azure-identity\",\n",
    "    \"python-dotenv\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    \n",
    "print(\"âœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38167e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Loaded .env from current directory (fallback)\n",
      "âœ… Azure OpenAI Endpoint: https://proj-afp2-resource.openai.azure.com/\n",
      "âœ… Deployment Name: gpt-4.1-mini\n",
      "âœ… Authentication: API Key\n"
     ]
    }
   ],
   "source": [
    "# Configuration: Load environment variables\n",
    "# Set up your Azure OpenAI connection\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def find_config_path(start_path: str) -> str:\n",
    "    \"\"\"Find the 'python' folder by traversing up from start_path.\"\"\"\n",
    "    current_dir = Path(start_path)\n",
    "    \n",
    "    while current_dir is not None:\n",
    "        if current_dir.name.lower() == \"python\":\n",
    "            return str(current_dir)\n",
    "        if current_dir.parent == current_dir:\n",
    "            break\n",
    "        current_dir = current_dir.parent\n",
    "    \n",
    "    # Fallback to start path if python folder not found\n",
    "    return start_path\n",
    "\n",
    "\n",
    "def load_env_file(env_path: str) -> dict:\n",
    "    \"\"\"Load environment variables from .env file (JSON format).\"\"\"\n",
    "    env_file = Path(env_path) / \".env\"\n",
    "    \n",
    "    if not env_file.exists():\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        with open(env_file, 'r') as f:\n",
    "            content = f.read()\n",
    "            env_vars = json.loads(content)\n",
    "            \n",
    "            # Set environment variables\n",
    "            for key, value in env_vars.items():\n",
    "                os.environ[key] = str(value)\n",
    "            \n",
    "            return env_vars\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: try loading as standard dotenv format\n",
    "        load_dotenv(env_file, override=True)\n",
    "        return {}\n",
    "    except IOError as e:\n",
    "        print(f\"Warning: Failed to load .env file: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Load environment variables from .env file in the python root folder\n",
    "config_path = find_config_path(os.getcwd())\n",
    "env_vars = load_env_file(config_path)\n",
    "if env_vars:\n",
    "    print(f\"âœ… Loaded {len(env_vars)} environment variables from: {config_path}/.env\")\n",
    "else:\n",
    "    # Fallback: try loading from current directory\n",
    "    load_dotenv()\n",
    "    print(\"âš ï¸ Loaded .env from current directory (fallback)\")\n",
    "\n",
    "# Check required configuration - support both naming conventions\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\") or os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\") or os.getenv(\"AZURE_AI_MODEL_DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
    "\n",
    "if endpoint:\n",
    "    print(f\"âœ… Azure OpenAI Endpoint: {endpoint}\")\n",
    "    print(f\"âœ… Deployment Name: {deployment}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Azure endpoint not set\")\n",
    "    print(\"Please set one of the following environment variables:\")\n",
    "    print(\"  - AZURE_AI_PROJECT_ENDPOINT or AZURE_OPENAI_ENDPOINT\")\n",
    "    print(\"And optionally:\")\n",
    "    print(\"  - AZURE_AI_MODEL_DEPLOYMENT_NAME or AZURE_OPENAI_DEPLOYMENT_NAME (default: gpt-4o-mini)\")\n",
    "    print(\"  - AZURE_OPENAI_API_KEY (for API key auth)\")\n",
    "\n",
    "# Check authentication method\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "tenant_id = os.getenv(\"AZURE_TENANT_ID\")\n",
    "client_id = os.getenv(\"AZURE_CLIENT_ID\")\n",
    "client_secret = os.getenv(\"AZURE_CLIENT_SECRET\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"âœ… Authentication: API Key\")\n",
    "elif tenant_id and client_id and client_secret:\n",
    "    print(\"âœ… Authentication: Service Principal\")\n",
    "else:\n",
    "    print(\"âœ… Authentication: Azure CLI / DefaultAzureCredential\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24c1b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created AzureOpenAI client with API Key auth\n",
      "   Endpoint: https://proj-afp2-resource.openai.azure.com/\n",
      "   Deployment: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Create Azure OpenAI Client\n",
    "# This demonstrates how to set up the client with different auth methods\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, AzureCliCredential, ClientSecretCredential, get_bearer_token_provider\n",
    "\n",
    "# Get configuration - AZURE_OPENAI takes priority over AZURE_AI_PROJECT\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
    "\n",
    "# Only fall back to Foundry endpoint if Azure OpenAI endpoint is not set\n",
    "if not endpoint:\n",
    "    endpoint = os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "    deployment = os.getenv(\"AZURE_AI_MODEL_DEPLOYMENT_NAME\", deployment)\n",
    "    print(\"âš ï¸ Using Azure AI Foundry endpoint (AZURE_AI_PROJECT_ENDPOINT)\")\n",
    "\n",
    "if not endpoint:\n",
    "    raise ValueError(\"Azure endpoint not set. Set AZURE_OPENAI_ENDPOINT environment variable.\")\n",
    "\n",
    "api_version = \"2024-02-15-preview\"\n",
    "\n",
    "# Use API Key if provided (standard Azure OpenAI)\n",
    "if api_key:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_key=api_key,\n",
    "        api_version=api_version\n",
    "    )\n",
    "    print(f\"âœ… Created AzureOpenAI client with API Key auth\")\n",
    "    print(f\"   Endpoint: {endpoint}\")\n",
    "    print(f\"   Deployment: {deployment}\")\n",
    "else:\n",
    "    # Token-based auth (for Foundry endpoints or when no API key)\n",
    "    print(\"âš ï¸ No API key found, using token-based authentication\")\n",
    "    try:\n",
    "        credential = AzureCliCredential()\n",
    "        token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=endpoint,\n",
    "            azure_ad_token_provider=token_provider,\n",
    "            api_version=api_version\n",
    "        )\n",
    "        print(\"âœ… Created AzureOpenAI client with Azure CLI auth\")\n",
    "    except Exception as e:\n",
    "        credential = DefaultAzureCredential()\n",
    "        token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=endpoint,\n",
    "            azure_ad_token_provider=token_provider,\n",
    "            api_version=api_version\n",
    "        )\n",
    "        print(\"âœ… Created AzureOpenAI client with DefaultAzureCredential\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2edf631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Configuration Check ===\n",
      "Endpoint: https://proj-afp2-resource.openai.azure.com/\n",
      "API Key: **********PLTk\n",
      "Deployment: gpt-4.1-mini\n",
      "âš ï¸ Endpoint should NOT end with '/' - this can cause auth errors\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check your configuration values\n",
    "import os\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\") or os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\") or os.getenv(\"AZURE_AI_MODEL_DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
    "\n",
    "print(\"=== Configuration Check ===\")\n",
    "print(f\"Endpoint: {endpoint}\")\n",
    "print(f\"API Key: {'*' * 10 + api_key[-4:] if api_key and len(api_key) > 4 else 'NOT SET or too short'}\")\n",
    "print(f\"Deployment: {deployment}\")\n",
    "\n",
    "# Validate endpoint format\n",
    "if endpoint:\n",
    "    if not endpoint.startswith(\"https://\"):\n",
    "        print(\"âš ï¸ Endpoint should start with 'https://'\")\n",
    "    if not \"openai.azure.com\" in endpoint:\n",
    "        print(\"âš ï¸ Endpoint should contain 'openai.azure.com'\")\n",
    "    if endpoint.endswith(\"/\"):\n",
    "        print(\"âš ï¸ Endpoint should NOT end with '/' - this can cause auth errors\")\n",
    "else:\n",
    "    print(\"âŒ Endpoint is not set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1abb4c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Azure OpenAI:\n",
      "  Hello from the MCP Lab!\n",
      "\n",
      "âœ… Azure OpenAI connection successful!\n",
      "  Model: gpt-4.1-mini-2025-04-14\n",
      "  Tokens: 37\n"
     ]
    }
   ],
   "source": [
    "# Test the Azure OpenAI connection\n",
    "# Send a simple completion request to verify everything is working\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Say 'Hello from MCP Lab!' in exactly 5 words.\"}\n",
    "    ],\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "print(\"Response from Azure OpenAI:\")\n",
    "print(f\"  {response.choices[0].message.content}\")\n",
    "print()\n",
    "print(f\"âœ… Azure OpenAI connection successful!\")\n",
    "print(f\"  Model: {response.model}\")\n",
    "print(f\"  Tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a13ecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MCP Server defined with tools\n",
      "Available tools:\n",
      "  - GetConfig: Gets a configuration value by key\n"
     ]
    }
   ],
   "source": [
    "# MCP Server Example - Define tools\n",
    "# This shows how to create MCP tools\n",
    "\n",
    "from mcp.server import Server\n",
    "from mcp.types import Tool, TextContent\n",
    "\n",
    "# Create an MCP server instance\n",
    "server = Server(\"example-server\")\n",
    "\n",
    "# Define a simple tool\n",
    "@server.list_tools()\n",
    "async def list_tools() -> list[Tool]:\n",
    "    \"\"\"List available tools.\"\"\"\n",
    "    return [\n",
    "        Tool(\n",
    "            name=\"GetConfig\",\n",
    "            description=\"Gets a configuration value by key\",\n",
    "            inputSchema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"key\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The configuration key\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"key\"]\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    "\n",
    "print(\"âœ… MCP Server defined with tools\")\n",
    "print(\"Available tools:\")\n",
    "tools = await list_tools()\n",
    "for tool in tools:\n",
    "    print(f\"  - {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89793512",
   "metadata": {},
   "source": [
    "## Closing Thoughts: From Concept to Implementation\n",
    "\n",
    "Having explored the MCP concepts and the complete solution implementation, here are the key takeaways from this lab:\n",
    "\n",
    "### Architecture Patterns You've Learned\n",
    "\n",
    "The solution demonstrates a **three-tier MCP architecture**:\n",
    "\n",
    "1. **Backend Layer** (`mcp_remote_server`) - A pure REST API using FastAPI that manages ticket data\n",
    "2. **Bridge Layer** (`mcp_bridge`) - An HTTP/SSE MCP server that wraps the REST API, exposing it through MCP protocol\n",
    "3. **Local Tools Layer** (`mcp_local_server`) - A STDIO MCP server providing direct configuration and ticket management\n",
    "\n",
    "### Transport Mechanisms in Action\n",
    "\n",
    "You've worked with both MCP transport types:\n",
    "\n",
    "- **STDIO Transport** (Local Server): Spawns Python subprocess, communicates via stdin/stdout. Perfect for local tools and CLI integrations.\n",
    "- **HTTP/SSE Transport** (Bridge): Runs as web service, enables remote access. Ideal for cloud services and shared tooling across multiple agents.\n",
    "\n",
    "### AI Agent Integration Pattern\n",
    "\n",
    "The `mcp_agent_client` demonstrates production-ready patterns:\n",
    "\n",
    "- **Multi-auth Support**: API Key, Service Principal (ClientSecretCredential), and Azure CLI fallback\n",
    "- **Interactive Sessions**: Chat loop with tool calling, maintaining conversation history\n",
    "- **Tool Discovery**: Dynamic tool listing and OpenAI function format conversion\n",
    "- **Error Handling**: Graceful degradation when services aren't available\n",
    "- **Menu-driven UX**: Clean separation between local and remote MCP demos\n",
    "\n",
    "### Real-World Application\n",
    "\n",
    "This architecture pattern enables:\n",
    "\n",
    "- **Existing API Integration**: Wrap any REST API with MCP without modifying the backend\n",
    "- **Hybrid Deployments**: Combine local tools (config management) with remote services (ticket systems)\n",
    "- **Agent Portability**: Same MCP client works with both STDIO and HTTP/SSE transports\n",
    "- **Tool Composition**: Agents can discover and use tools dynamically at runtime\n",
    "\n",
    "### Next Steps in Your Journey\n",
    "\n",
    "1. **Extend the Local Server**: Add more tools (file operations, system commands, database queries)\n",
    "2. **Secure the Bridge**: Add authentication, rate limiting, and request validation\n",
    "3. **Multi-Server Agents**: Connect your agent to multiple MCP servers simultaneously\n",
    "4. **Custom Protocols**: Bridge other protocols (GraphQL, gRPC, WebSockets) to MCP\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**MCP is a universal adapter layer** that lets AI agents interact with any tool or data source through a standardized protocol. By separating concerns (backend logic, MCP exposure, agent interaction), you create maintainable, scalable, and composable AI systems.\n",
    "\n",
    "The code examples in this notebook provide the foundation. The solution in `../solution/` shows how these pieces come together into a complete, production-ready system with proper error handling, authentication, and user experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
